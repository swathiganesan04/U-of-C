{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3313,"status":"ok","timestamp":1684749264631,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"_B9y4mmd5TxA","outputId":"151f5ade-c483-4766-baf5-9ebb1fb94cd2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"t7TCgzqQ2WY2"},"source":["### Importing libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"YKzEfSHt4316","executionInfo":{"status":"ok","timestamp":1684749270868,"user_tz":300,"elapsed":5887,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"}}},"outputs":[],"source":["from traitlets.traitlets import validate\n","import pandas as pd\n","import os\n","import json\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","from pathlib import Path\n","import glob"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9454,"status":"ok","timestamp":1684749280318,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"qzcUcKoo_17S","outputId":"8fa82b8b-0f4d-48f7-c9aa-f9b1b095e6ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/rcmalli/keras-vggface.git\n","  Cloning https://github.com/rcmalli/keras-vggface.git to /tmp/pip-req-build-cb9q7rj6\n","  Running command git clone --filter=blob:none --quiet https://github.com/rcmalli/keras-vggface.git /tmp/pip-req-build-cb9q7rj6\n","  Resolved https://github.com/rcmalli/keras-vggface.git to commit bee35376e76e35d00aeec503f2f242611a97b38a\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras-vggface==0.6) (1.22.4)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.10/dist-packages (from keras-vggface==0.6) (1.10.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-vggface==0.6) (3.8.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from keras-vggface==0.6) (8.4.0)\n","Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-vggface==0.6) (2.12.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras-vggface==0.6) (1.16.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from keras-vggface==0.6) (6.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: keras_applications in /usr/local/lib/python3.10/dist-packages (1.0.8)\n"]}],"source":["#https://shubhasmitaroy.medium.com/for-those-who-get-an-error-on-importing-vggface-related-packages-e5afdd0c3f01\n","!pip install git+https://github.com/rcmalli/keras-vggface.git\n","!pip install keras_applications --no-deps\n","filename = \"/usr/local/lib/python3.10/dist-packages/keras_vggface/models.py\"\n","text = open(filename).read()\n","open(filename, \"w+\").write(text.replace('keras.engine.topology', 'tensorflow.keras.utils'))\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"iClQGFNi8VLt","executionInfo":{"status":"ok","timestamp":1684749280543,"user_tz":300,"elapsed":229,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"}}},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","from keras_vggface.vggface import VGGFace\n","from keras.layers import Dense, GlobalAveragePooling2D\n","from keras.models import Model\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n"]},{"cell_type":"code","source":["path = \"/content/gdrive/MyDrive/Colab Notebooks/Data/\""],"metadata":{"id":"4S5LnJQe2wh_","executionInfo":{"status":"ok","timestamp":1684749280543,"user_tz":300,"elapsed":3,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WxyjoRMN2WY5"},"source":["### Defining data paths"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":752,"status":"ok","timestamp":1684749281293,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"gIAtmppW44Ye","outputId":"187b5251-ef26-40ac-dc64-ef25bc13c099"},"outputs":[{"output_type":"stream","name":"stdout","text":["train data dimension: (3203, 3)\n","test data dimension:  (750, 3)\n"]}],"source":["train_processed_dir = path+'Train/train_aligned/'\n","test_processed_dir = path+'Test/test_aligned/'\n","\n","# remove rows that no face images\n","train_img = os.listdir(train_processed_dir)\n","test_img = os.listdir(test_processed_dir)\n","\n","train = pd.read_csv(path+'Train/train.csv')\n","valid = pd.read_csv(path+'Test/valid.csv')\n","\n","train = train.loc[train['name'].isin(train_img),:]\n","valid = valid.loc[valid['name'].isin(test_img),:]\n","\n","print('train data dimension: {}'.format(str(train.shape)))\n","print('test data dimension:  {}'.format(str(valid.shape)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVUuZSN_68AA"},"outputs":[],"source":["# create metrics, model dirs\n","Path(path+'/metrics').mkdir(parents = True, exist_ok = True)\n","Path(path+'/saved_model').mkdir(parents = True, exist_ok = True)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1684749281294,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"OA2XllD98B5W","outputId":"b98674d7-e9fd-4da3-d209-06b2218b3044"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   gender        bmi       name\n","0       0  34.207396  img_0.bmp\n","1       0  26.453720  img_1.bmp\n","2       1  34.967561  img_2.bmp\n","3       1  22.044766  img_3.bmp\n","4       1  25.845588  img_6.bmp"],"text/html":["\n","  <div id=\"df-a7b442e3-0576-4536-b53f-89930e28a528\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>gender</th>\n","      <th>bmi</th>\n","      <th>name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>34.207396</td>\n","      <td>img_0.bmp</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>26.453720</td>\n","      <td>img_1.bmp</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>34.967561</td>\n","      <td>img_2.bmp</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>22.044766</td>\n","      <td>img_3.bmp</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>25.845588</td>\n","      <td>img_6.bmp</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7b442e3-0576-4536-b53f-89930e28a528')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a7b442e3-0576-4536-b53f-89930e28a528 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a7b442e3-0576-4536-b53f-89930e28a528');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}],"source":["data = pd.concat([train, valid])\n","data[['gender','bmi','name']].head()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8070,"status":"ok","timestamp":1684749289359,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"2xu_ilO3o6l-","outputId":"5a64a543-988a-47aa-e0fb-4fc380693db7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: mtcnn in /usr/local/lib/python3.10/dist-packages (0.1.1)\n","Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (2.12.0)\n","Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (4.7.0.72)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.22.4)\n"]}],"source":["!pip install mtcnn"]},{"cell_type":"markdown","metadata":{"id":"0qVOqR_U2WY7"},"source":["### Data Augmentation and Batch Generation "]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1159,"status":"ok","timestamp":1684749320693,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"GuY4loFY_tlG","outputId":"6b1b5ce4-3edd-4118-c697-2c987f671d78"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 3203 validated image filenames.\n","Found 750 validated image filenames.\n"]}],"source":["# Create an ImageDataGenerator for data augmentation\n","datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Generate batches of training images and labels\n","train_generator = datagen.flow_from_dataframe(\n","    dataframe=train,\n","    directory=train_processed_dir,\n","    x_col='name',\n","    y_col=['bmi', 'gender'],\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='multi_output',\n","    shuffle=True\n",")\n","\n","\n","# Generate batches of validation images and labels\n","valid_generator = datagen.flow_from_dataframe(\n","    dataframe=valid,\n","    directory=test_processed_dir,\n","    x_col='name',\n","    y_col=['bmi', 'gender'],\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='multi_output',\n","    shuffle=False\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"63qpJhCa2WY7"},"source":["### Model 1 : VGGFace model with ResNet50 and 64 fully connected layers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4968,"status":"ok","timestamp":1684197889160,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"3ALj3w_cAd-S","outputId":"749fdda0-3a86-4013-d032-1cee9a08a886"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_resnet50.h5\n","94694792/94694792 [==============================] - 1s 0us/step\n"]}],"source":["# Load the pre-trained VGGFace model\n","base_model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3))\n","\n","# Add a global spatial average pooling layer\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","\n","# Add fully connected layers for BMI prediction\n","bmi_fc = Dense(64, activation='relu')(x)\n","bmi_fc = Dense(1, name='bmi')(bmi_fc)\n","\n","# Add fully connected layers for gender prediction\n","gender_fc = Dense(64, activation='relu')(x)\n","gender_fc = Dense(1, activation='sigmoid', name='gender')(gender_fc)\n","\n","# Create the fine-tuned model\n","model = Model(inputs=base_model.input, outputs=[bmi_fc, gender_fc])\n","\n","# Freeze the weights of the pre-trained layers\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","# Compile the model with appropriate loss functions and metrics\n","model.compile(optimizer=Adam(), \n","              loss={'bmi': 'mean_squared_error', 'gender': 'binary_crossentropy'},\n","              metrics={'bmi': 'mse', 'gender': 'accuracy'})\n","\n","from keras.callbacks import EarlyStopping\n","\n","# Train the model\n","model.fit(train_generator,\n","          steps_per_epoch=len(train_generator),\n","          epochs=10,\n","          validation_data=valid_generator,\n","          validation_steps=len(valid_generator),\n","          callbacks=[EarlyStopping(patience=2)])\n","\n","model.save(path+'saved_model/fine_tuned_vggface_model_64.h5')\n"]},{"cell_type":"markdown","metadata":{"id":"yTND_Kh5fPLv"},"source":["##### Results\n","\n","Epoch 1/10\n","101/101 [==============================] - 871s 9s/step - loss: 261.4586 - bmi_loss: 260.7733 - gender_loss: 0.6853 - bmi_mse: 260.7733 - gender_accuracy: 0.5832 - val_loss: 86.8630 - val_bmi_loss: 86.1779 - val_gender_loss: 0.6851 - val_bmi_mse: 86.1779 - val_gender_accuracy: 0.5680\n","\n","Epoch 2/10\n","101/101 [==============================] - 714s 7s/step - loss: 64.9482 - bmi_loss: 64.2684 - gender_loss: 0.6797 - bmi_mse: 64.2684 - gender_accuracy: 0.5916 - val_loss: 86.1564 - val_bmi_loss: 85.4725 - val_gender_loss: 0.6839 - val_bmi_mse: 85.4725 - val_gender_accuracy: 0.5680\n","\n","Epoch 3/10\n","101/101 [==============================] - 721s 7s/step - loss: 65.1290 - bmi_loss: 64.4527 - gender_loss: 0.6763 - bmi_mse: 64.4527 - gender_accuracy: 0.5935 - val_loss: 86.8603 - val_bmi_loss: 86.1715 - val_gender_loss: 0.6888 - val_bmi_mse: 86.1715 - val_gender_accuracy: 0.5680\n","\n","Epoch 4/10\n","101/101 [==============================] - 717s 7s/step - loss: 65.1406 - bmi_loss: 64.4598 - gender_loss: 0.6808 - bmi_mse: 64.4598 - gender_accuracy: 0.5798 - val_loss: 88.2110 - val_bmi_loss: 87.5222 - val_gender_loss: 0.6888 - val_bmi_mse: 87.5222 - val_gender_accuracy: 0.5680"]},{"cell_type":"markdown","metadata":{"id":"7FIKAmQt0LMh"},"source":["#### Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4391,"status":"ok","timestamp":1684211786878,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"YwWYhrq0Oeh9","outputId":"4258be9c-fe11-4918-b0e5-4ad8084d46f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 1s/step\n","Predicted BMI: [50.503872]\n","Predicted Gender: Male\n"]}],"source":["from keras.models import load_model\n","from IPython.display import Image\n","\n","image_name = 'img_3417.bmp'\n","image_path = path+'Test/Test_Img/'\n","\n","# Load the saved model\n","model = load_model(path+'saved_model/fine_tuned_vggface_model_64.h5')\n","\n","from keras_vggface.utils import preprocess_input\n","from PIL import Image\n","\n","# Load and preprocess the input image\n","input_image = Image.open(image_path+image_name)\n","input_image = input_image.resize((224, 224))  # Resize to the input shape of the model\n","input_image = np.array(input_image).astype(np.float32)  # Convert to float32\n","input_image = preprocess_input(input_image)  # Preprocess the image\n","\n","# Predict BMI and gender\n","prediction = model.predict(np.expand_dims(input_image, axis=0))\n","\n","# Retrieve the predicted BMI and gender values\n","predicted_bmi = prediction[0][0]\n","predicted_gender = 'Male' if prediction[1][0] > 0.5 else 'Female'\n","\n","# Print the predicted BMI and gender\n","print(\"Predicted BMI:\", predicted_bmi)\n","print(\"Predicted Gender:\", predicted_gender)\n","\n","#22.49458302\tFemale\t0\timg_3417.bmp"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":143496,"status":"ok","timestamp":1684211934551,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"2Ex34c99o7yW","outputId":"55b71658-60e8-4d04-b0a3-61456240f4d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Frozen layers with 64 fully connected dense layers\n","24/24 [==============================] - 135s 6s/step - loss: 88.2110 - bmi_loss: 87.5222 - gender_loss: 0.6888 - bmi_mse: 87.5222 - gender_accuracy: 0.5680\n"]},{"data":{"text/plain":["[88.21102142333984,\n"," 87.52223205566406,\n"," 0.6887925863265991,\n"," 87.52223205566406,\n"," 0.5680000185966492]"]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Frozen layers with 64 fully connected dense layers\")\n","model.evaluate(valid_generator)\n"]},{"cell_type":"markdown","metadata":{"id":"rvagEMVv2WY9"},"source":["### Model 2 : VGGFace model with ResNet50 and 128 fully connected layers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6012952,"status":"ok","timestamp":1684211134551,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"PtBOy84LBD_d","outputId":"792c75e4-af0e-49fa-c35c-b6e95ec968d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","101/101 [==============================] - 721s 7s/step - loss: 327.5560 - bmi_loss: 326.8747 - gender_loss: 0.6813 - bmi_mse: 326.8747 - gender_accuracy: 0.5791 - val_loss: 86.1752 - val_bmi_loss: 85.4357 - val_gender_loss: 0.7395 - val_bmi_mse: 85.4357 - val_gender_accuracy: 0.5680\n","Epoch 2/20\n","101/101 [==============================] - 785s 8s/step - loss: 64.9085 - bmi_loss: 64.2234 - gender_loss: 0.6851 - bmi_mse: 64.2234 - gender_accuracy: 0.5757 - val_loss: 87.4735 - val_bmi_loss: 86.7767 - val_gender_loss: 0.6967 - val_bmi_mse: 86.7767 - val_gender_accuracy: 0.5680\n","Epoch 3/20\n","101/101 [==============================] - 716s 7s/step - loss: 64.9831 - bmi_loss: 64.2989 - gender_loss: 0.6842 - bmi_mse: 64.2989 - gender_accuracy: 0.5851 - val_loss: 85.8994 - val_bmi_loss: 85.1724 - val_gender_loss: 0.7270 - val_bmi_mse: 85.1724 - val_gender_accuracy: 0.5680\n","Epoch 4/20\n","101/101 [==============================] - 712s 7s/step - loss: 65.0394 - bmi_loss: 64.3626 - gender_loss: 0.6768 - bmi_mse: 64.3626 - gender_accuracy: 0.5929 - val_loss: 88.7103 - val_bmi_loss: 88.0230 - val_gender_loss: 0.6874 - val_bmi_mse: 88.0230 - val_gender_accuracy: 0.5680\n","Epoch 5/20\n","101/101 [==============================] - 785s 8s/step - loss: 65.0803 - bmi_loss: 64.3991 - gender_loss: 0.6811 - bmi_mse: 64.3991 - gender_accuracy: 0.5810 - val_loss: 87.3919 - val_bmi_loss: 86.6971 - val_gender_loss: 0.6949 - val_bmi_mse: 86.6971 - val_gender_accuracy: 0.5680\n","Epoch 6/20\n","101/101 [==============================] - 713s 7s/step - loss: 64.9563 - bmi_loss: 64.2804 - gender_loss: 0.6759 - bmi_mse: 64.2804 - gender_accuracy: 0.6007 - val_loss: 86.5703 - val_bmi_loss: 85.8784 - val_gender_loss: 0.6919 - val_bmi_mse: 85.8784 - val_gender_accuracy: 0.5747\n","Epoch 7/20\n","101/101 [==============================] - 702s 7s/step - loss: 65.1072 - bmi_loss: 64.4304 - gender_loss: 0.6768 - bmi_mse: 64.4304 - gender_accuracy: 0.6007 - val_loss: 87.9593 - val_bmi_loss: 87.2691 - val_gender_loss: 0.6902 - val_bmi_mse: 87.2691 - val_gender_accuracy: 0.5680\n","Epoch 8/20\n","101/101 [==============================] - 716s 7s/step - loss: 65.0734 - bmi_loss: 64.3937 - gender_loss: 0.6797 - bmi_mse: 64.3937 - gender_accuracy: 0.5919 - val_loss: 93.5737 - val_bmi_loss: 92.8766 - val_gender_loss: 0.6971 - val_bmi_mse: 92.8766 - val_gender_accuracy: 0.5680\n"]}],"source":["# Load the pre-trained VGGFace model\n","base_model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3))\n","\n","# Add a global average pooling layer\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","\n","# Add fully connected layers for BMI prediction\n","bmi_fc = Dense(128, activation='relu')(x)\n","bmi_fc = Dense(1, name='bmi')(bmi_fc)\n","\n","# Add fully connected layers for gender prediction\n","gender_fc = Dense(128, activation='relu')(x)\n","gender_fc = Dense(1, activation='sigmoid', name='gender')(gender_fc)\n","\n","# Create the fine-tuned model\n","model = Model(inputs=base_model.input, outputs=[bmi_fc, gender_fc])\n","\n","# Freeze the weights of the pre-trained layers\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","# Compile the model with appropriate loss functions and metrics\n","model.compile(optimizer=Adam(), \n","              loss={'bmi': 'mean_squared_error', 'gender': 'binary_crossentropy'},\n","              metrics={'bmi': 'mse', 'gender': 'accuracy'})\n","\n","# Train the model\n","model.fit(train_generator,\n","          steps_per_epoch=len(train_generator),\n","          epochs=20,\n","          validation_data=valid_generator,\n","          validation_steps=len(valid_generator),\n","          callbacks=[EarlyStopping(patience=5)])\n","\n","# Save the fine-tuned model\n","model.save(path+'saved_model/fine_tuned_vggface_model_128.h5')\n"]},{"cell_type":"markdown","metadata":{"id":"FIqz4fBY2WY9"},"source":["#### Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":134113,"status":"ok","timestamp":1684211567744,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"IcYIZPmD0ufD","outputId":"ba9ce7a1-9ad6-487a-a6e9-381edac3d40e"},"outputs":[{"name":"stdout","output_type":"stream","text":["24/24 [==============================] - 133s 6s/step - loss: 93.5737 - bmi_loss: 92.8766 - gender_loss: 0.6971 - bmi_mse: 92.8766 - gender_accuracy: 0.5680\n"]},{"data":{"text/plain":["[93.57368469238281,\n"," 92.87657165527344,\n"," 0.6971324682235718,\n"," 92.87657165527344,\n"," 0.5680000185966492]"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Frozen layers with 128 fully connected dense layers and 20 epochs with early stopping stop training if it doesn't improve for 5 consecutive epochs\")\n","model.evaluate(valid_generator)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9393,"status":"ok","timestamp":1684211714911,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"4IssS2E_1v8a","outputId":"abe506f9-c039-4291-d08d-9e8497e0ee5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 1s/step\n","Predicted BMI: [43.087055]\n","Predicted Gender: Male\n"]}],"source":["from keras.models import load_model\n","\n","# Load the saved model\n","model = load_model(path+'saved_model/fine_tuned_vggface_model_128.h5')\n","\n","from keras_vggface.utils import preprocess_input\n","from PIL import Image\n","\n","# Load and preprocess the input image\n","input_image = Image.open(image_path+image_name)\n","input_image = input_image.resize((224, 224))  # Resize to the input shape of the model\n","input_image = np.array(input_image).astype(np.float32)  # Convert to float32\n","input_image = preprocess_input(input_image)  # Preprocess the image\n","\n","# Predict BMI and gender\n","prediction = model.predict(np.expand_dims(input_image, axis=0))\n","\n","# Retrieve the predicted BMI and gender values\n","predicted_bmi = prediction[0][0]\n","predicted_gender = 'Male' if prediction[1][0] > 0.5 else 'Female'\n","\n","# Print the predicted BMI and gender\n","print(\"Predicted BMI:\", predicted_bmi)\n","print(\"Predicted Gender:\", predicted_gender)\n","\n","#22.49458302\tFemale\t0\timg_3417.bmp"]},{"cell_type":"markdown","metadata":{"id":"l0jxFhrR2WY-"},"source":["### Model 3 : VGGFace model with ResNet50 and 128 fully connected layers w unfreezing last 5 layers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8697791,"status":"ok","timestamp":1684251225038,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"P7BYHPFDEXkS","outputId":"a2aed9ad-ee37-4ef9-f5a6-6bd3827a1923"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","101/101 [==============================] - 2851s 28s/step - loss: 91.5627 - bmi_loss: 90.7616 - gender_loss: 0.8011 - bmi_mse: 90.7616 - gender_accuracy: 0.5882 - val_loss: 271465.2188 - val_bmi_loss: 271464.4062 - val_gender_loss: 0.8515 - val_bmi_mse: 271464.4062 - val_gender_accuracy: 0.4320\n","Epoch 2/20\n","101/101 [==============================] - 2814s 28s/step - loss: 65.2943 - bmi_loss: 64.6231 - gender_loss: 0.6712 - bmi_mse: 64.6231 - gender_accuracy: 0.6007 - val_loss: 98.3422 - val_bmi_loss: 97.6350 - val_gender_loss: 0.7072 - val_bmi_mse: 97.6350 - val_gender_accuracy: 0.5680\n","Epoch 3/20\n","101/101 [==============================] - 2844s 28s/step - loss: 57.9673 - bmi_loss: 57.2851 - gender_loss: 0.6821 - bmi_mse: 57.2851 - gender_accuracy: 0.6007 - val_loss: 504.5482 - val_bmi_loss: 503.8555 - val_gender_loss: 0.6927 - val_bmi_mse: 503.8555 - val_gender_accuracy: 0.5680\n","Epoch 4/20\n","101/101 [==============================] - 2838s 28s/step - loss: 56.3396 - bmi_loss: 55.6617 - gender_loss: 0.6779 - bmi_mse: 55.6617 - gender_accuracy: 0.6007 - val_loss: 855.2351 - val_bmi_loss: 854.4608 - val_gender_loss: 0.7743 - val_bmi_mse: 854.4608 - val_gender_accuracy: 0.5680\n","Epoch 5/20\n","101/101 [==============================] - 2813s 28s/step - loss: 53.9920 - bmi_loss: 53.3195 - gender_loss: 0.6725 - bmi_mse: 53.3195 - gender_accuracy: 0.6007 - val_loss: 84.1893 - val_bmi_loss: 83.5015 - val_gender_loss: 0.6878 - val_bmi_mse: 83.5015 - val_gender_accuracy: 0.5680\n","Epoch 6/20\n","101/101 [==============================] - 2707s 27s/step - loss: 50.8365 - bmi_loss: 50.1657 - gender_loss: 0.6708 - bmi_mse: 50.1657 - gender_accuracy: 0.6007 - val_loss: 81.2866 - val_bmi_loss: 80.5921 - val_gender_loss: 0.6944 - val_bmi_mse: 80.5921 - val_gender_accuracy: 0.5680\n","Epoch 7/20\n","101/101 [==============================] - 2725s 27s/step - loss: 49.0457 - bmi_loss: 48.3757 - gender_loss: 0.6700 - bmi_mse: 48.3757 - gender_accuracy: 0.6007 - val_loss: 167.8343 - val_bmi_loss: 167.0896 - val_gender_loss: 0.7447 - val_bmi_mse: 167.0896 - val_gender_accuracy: 0.5680\n","Epoch 8/20\n","101/101 [==============================] - 2722s 27s/step - loss: 48.8857 - bmi_loss: 48.2195 - gender_loss: 0.6662 - bmi_mse: 48.2195 - gender_accuracy: 0.6007 - val_loss: 135.2918 - val_bmi_loss: 134.5611 - val_gender_loss: 0.7308 - val_bmi_mse: 134.5611 - val_gender_accuracy: 0.5680\n","Epoch 9/20\n","101/101 [==============================] - 2759s 27s/step - loss: 42.2368 - bmi_loss: 41.5745 - gender_loss: 0.6623 - bmi_mse: 41.5745 - gender_accuracy: 0.6007 - val_loss: 70.1336 - val_bmi_loss: 69.4678 - val_gender_loss: 0.6658 - val_bmi_mse: 69.4678 - val_gender_accuracy: 0.5680\n","Epoch 10/20\n","101/101 [==============================] - 2743s 27s/step - loss: 37.9148 - bmi_loss: 37.2546 - gender_loss: 0.6602 - bmi_mse: 37.2546 - gender_accuracy: 0.6007 - val_loss: 71.9913 - val_bmi_loss: 71.3135 - val_gender_loss: 0.6778 - val_bmi_mse: 71.3135 - val_gender_accuracy: 0.5680\n","Epoch 11/20\n","101/101 [==============================] - 2718s 27s/step - loss: 33.1379 - bmi_loss: 32.4746 - gender_loss: 0.6634 - bmi_mse: 32.4746 - gender_accuracy: 0.6007 - val_loss: 80.8497 - val_bmi_loss: 80.1665 - val_gender_loss: 0.6832 - val_bmi_mse: 80.1665 - val_gender_accuracy: 0.5680\n","Epoch 12/20\n","101/101 [==============================] - 2736s 27s/step - loss: 30.3731 - bmi_loss: 29.7200 - gender_loss: 0.6531 - bmi_mse: 29.7200 - gender_accuracy: 0.6007 - val_loss: 86.5185 - val_bmi_loss: 85.8305 - val_gender_loss: 0.6880 - val_bmi_mse: 85.8305 - val_gender_accuracy: 0.5680\n","Epoch 13/20\n","101/101 [==============================] - 2732s 27s/step - loss: 27.0570 - bmi_loss: 26.4008 - gender_loss: 0.6562 - bmi_mse: 26.4008 - gender_accuracy: 0.6007 - val_loss: 73.2865 - val_bmi_loss: 72.6310 - val_gender_loss: 0.6556 - val_bmi_mse: 72.6310 - val_gender_accuracy: 0.5680\n","Epoch 14/20\n","101/101 [==============================] - 2751s 27s/step - loss: 19.4087 - bmi_loss: 18.7600 - gender_loss: 0.6487 - bmi_mse: 18.7600 - gender_accuracy: 0.6007 - val_loss: 76.6980 - val_bmi_loss: 76.0428 - val_gender_loss: 0.6552 - val_bmi_mse: 76.0428 - val_gender_accuracy: 0.5680\n"]}],"source":["# Load the pre-trained VGGFace model\n","base_model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3))\n","\n","# Unfreeze and train the last few layers of the base model\n","for layer in base_model.layers[-5:]:\n","    layer.trainable = True\n","\n","# Add a global average pooling layer\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","\n","# Add fully connected layers for BMI prediction\n","bmi_fc = Dense(128, activation='relu')(x)\n","bmi_fc = Dense(1, name='bmi')(bmi_fc)\n","\n","# Add fully connected layers for gender prediction\n","gender_fc = Dense(128, activation='relu')(x)\n","gender_fc = Dense(1, activation='sigmoid', name='gender')(gender_fc)\n","\n","# Create the fine-tuned model\n","model = Model(inputs=base_model.input, outputs=[bmi_fc, gender_fc])\n","\n","# Compile the model with appropriate loss functions and metrics\n","model.compile(optimizer=Adam(), \n","              loss={'bmi': 'mean_squared_error', 'gender': 'binary_crossentropy'},\n","              metrics={'bmi': 'mse', 'gender': 'accuracy'})\n","\n","# Train the model\n","model.fit(train_generator,\n","          steps_per_epoch=len(train_generator),\n","          epochs=20,\n","          validation_data=valid_generator,\n","          validation_steps=len(valid_generator),\n","          callbacks=[EarlyStopping(patience=5)])\n","\n","# Save the fine-tuned model\n","model.save(path+'saved_model/fine_tuned_vggface_model_unfreeze5.h5')\n"]},{"cell_type":"markdown","metadata":{"id":"7Zq9nB_U2WY-"},"source":["#### validation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":143526,"status":"ok","timestamp":1684251398785,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"Sdz6PvIa3VB-","outputId":"231c764e-9496-4aa1-a379-c46f05242e48"},"outputs":[{"name":"stdout","output_type":"stream","text":["Unfreeze 5 layers with 128 fully connected dense layers and 20 epochs with early stopping stop training if it doesn't improve for 5 consecutive epochs\n","24/24 [==============================] - 133s 5s/step - loss: 76.6980 - bmi_loss: 76.0428 - gender_loss: 0.6552 - bmi_mse: 76.0428 - gender_accuracy: 0.5680\n"]},{"data":{"text/plain":["[76.69803619384766,\n"," 76.0428237915039,\n"," 0.6552107334136963,\n"," 76.0428237915039,\n"," 0.5680000185966492]"]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Unfreeze 5 layers with 128 fully connected dense layers and 20 epochs with early stopping stop training if it doesn't improve for 5 consecutive epochs\")\n","model.evaluate(valid_generator)"]},{"cell_type":"markdown","metadata":{"id":"hmwsOWYK2WY-"},"source":["### Model 4 : VGGFace model with VGG16 architecture and 64 fully connected layers "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17449590,"status":"ok","timestamp":1684280924433,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"2dASdCnYV-xJ","outputId":"8c1e8350-0350-40d7-b3ee-c10957b12b48"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_vgg16.h5\n","58909280/58909280 [==============================] - 1s 0us/step\n","Epoch 1/10\n","101/101 [==============================] - 2602s 26s/step - loss: 915.5443 - bmi_loss: 915.0029 - gender_loss: 0.5414 - bmi_mse: 915.0029 - gender_accuracy: 0.7705 - val_loss: 733.5018 - val_bmi_loss: 733.1183 - val_gender_loss: 0.3834 - val_bmi_mse: 733.1183 - val_gender_accuracy: 0.9253\n","Epoch 2/10\n","101/101 [==============================] - 2578s 26s/step - loss: 378.4223 - bmi_loss: 378.1328 - gender_loss: 0.2896 - bmi_mse: 378.1328 - gender_accuracy: 0.9223 - val_loss: 233.6090 - val_bmi_loss: 233.3849 - val_gender_loss: 0.2241 - val_bmi_mse: 233.3849 - val_gender_accuracy: 0.9413\n","Epoch 3/10\n","101/101 [==============================] - 2563s 25s/step - loss: 129.3594 - bmi_loss: 129.1618 - gender_loss: 0.1976 - bmi_mse: 129.1618 - gender_accuracy: 0.9419 - val_loss: 153.0738 - val_bmi_loss: 152.8917 - val_gender_loss: 0.1821 - val_bmi_mse: 152.8917 - val_gender_accuracy: 0.9413\n","Epoch 4/10\n","101/101 [==============================] - 2486s 25s/step - loss: 109.8024 - bmi_loss: 109.6432 - gender_loss: 0.1592 - bmi_mse: 109.6432 - gender_accuracy: 0.9494 - val_loss: 146.3227 - val_bmi_loss: 146.1623 - val_gender_loss: 0.1603 - val_bmi_mse: 146.1623 - val_gender_accuracy: 0.9453\n","Epoch 5/10\n","101/101 [==============================] - 2481s 25s/step - loss: 105.6060 - bmi_loss: 105.4670 - gender_loss: 0.1389 - bmi_mse: 105.4670 - gender_accuracy: 0.9566 - val_loss: 141.3773 - val_bmi_loss: 141.2279 - val_gender_loss: 0.1494 - val_bmi_mse: 141.2279 - val_gender_accuracy: 0.9507\n","Epoch 6/10\n","101/101 [==============================] - 2476s 25s/step - loss: 101.3372 - bmi_loss: 101.2101 - gender_loss: 0.1271 - bmi_mse: 101.2101 - gender_accuracy: 0.9603 - val_loss: 135.8628 - val_bmi_loss: 135.7136 - val_gender_loss: 0.1491 - val_bmi_mse: 135.7136 - val_gender_accuracy: 0.9467\n","Epoch 7/10\n","101/101 [==============================] - 2423s 24s/step - loss: 97.0323 - bmi_loss: 96.9130 - gender_loss: 0.1194 - bmi_mse: 96.9130 - gender_accuracy: 0.9603 - val_loss: 130.7616 - val_bmi_loss: 130.6222 - val_gender_loss: 0.1394 - val_bmi_mse: 130.6222 - val_gender_accuracy: 0.9547\n","Epoch 8/10\n","101/101 [==============================] - 2426s 24s/step - loss: 92.8556 - bmi_loss: 92.7453 - gender_loss: 0.1103 - bmi_mse: 92.7453 - gender_accuracy: 0.9647 - val_loss: 126.1497 - val_bmi_loss: 126.0121 - val_gender_loss: 0.1376 - val_bmi_mse: 126.0121 - val_gender_accuracy: 0.9547\n","Epoch 9/10\n","101/101 [==============================] - 2460s 24s/step - loss: 88.6326 - bmi_loss: 88.5279 - gender_loss: 0.1046 - bmi_mse: 88.5279 - gender_accuracy: 0.9672 - val_loss: 121.1148 - val_bmi_loss: 120.9766 - val_gender_loss: 0.1382 - val_bmi_mse: 120.9766 - val_gender_accuracy: 0.9493\n","Epoch 10/10\n","101/101 [==============================] - 2462s 24s/step - loss: 84.6387 - bmi_loss: 84.5366 - gender_loss: 0.1021 - bmi_mse: 84.5366 - gender_accuracy: 0.9669 - val_loss: 116.8602 - val_bmi_loss: 116.7264 - val_gender_loss: 0.1338 - val_bmi_mse: 116.7264 - val_gender_accuracy: 0.9587\n"]}],"source":["# Load the pre-trained VGGFace model\n","base_model = VGGFace(model='vgg16', include_top=False, input_shape=(224, 224, 3))\n","\n","# Add a global spatial average pooling layer\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","\n","# Add fully connected layers for BMI prediction\n","bmi_fc = Dense(64, activation='relu')(x)\n","bmi_fc = Dense(1, name='bmi')(bmi_fc)\n","\n","# Add fully connected layers for gender prediction\n","gender_fc = Dense(64, activation='relu')(x)\n","gender_fc = Dense(1, activation='sigmoid', name='gender')(gender_fc)\n","\n","# Create the fine-tuned model\n","model = Model(inputs=base_model.input, outputs=[bmi_fc, gender_fc])\n","\n","# Freeze the weights of the pre-trained layers\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","# Compile the model with appropriate loss functions and metrics\n","model.compile(optimizer=Adam(), \n","              loss={'bmi': 'mean_squared_error', 'gender': 'binary_crossentropy'},\n","              metrics={'bmi': 'mse', 'gender': 'accuracy'})\n","\n","from keras.callbacks import EarlyStopping\n","\n","# Train the model\n","model.fit(train_generator,\n","          steps_per_epoch=len(train_generator),\n","          epochs=10,\n","          validation_data=valid_generator,\n","          validation_steps=len(valid_generator),\n","          callbacks=[EarlyStopping(patience=5)])\n","\n","model.save(path+'saved_model/fine_tuned_vggface_model_vgg16.h5')\n"]},{"cell_type":"markdown","metadata":{"id":"phz-8YQt2WY-"},"source":["#### Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":509846,"status":"ok","timestamp":1684284795006,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"ogybBmHa3QHd","outputId":"01f1fe52-8b1e-4995-db26-fffd649bfb8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Unfreeze 5 layers with 128 fully connected dense layers and 20 epochs with early stopping stop training if it doesn't improve for 5 consecutive epochs\n","24/24 [==============================] - 484s 20s/step - loss: 116.8602 - bmi_loss: 116.7264 - gender_loss: 0.1338 - bmi_mse: 116.7264 - gender_accuracy: 0.9587\n"]},{"data":{"text/plain":["[116.8602294921875,\n"," 116.7264175415039,\n"," 0.1338060051202774,\n"," 116.7264175415039,\n"," 0.9586666822433472]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["model = load_model(path+'saved_model/fine_tuned_vggface_model_vgg16.h5')\n","\n","print(\"VGG16 architecture w Frozen layers with 64 fully connected dense layers\")\n","model.evaluate(valid_generator)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_G7xwNo5fjAE"},"outputs":[],"source":["from keras_vggface import utils\n","from keras.utils import load_img, img_to_array\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from PIL import Image\n","import numpy as np\n","\n","def crop_img(im,x,y,w,h):\n","    return im[y:(y+h),x:(x+w),:]\n","\n","def detect_face(face_path):\n","    img = cv2.cvtColor(cv2.imread(face_path), cv2.COLOR_BGR2RGB)\n","    box = detector.detect_faces(img)\n","    return box\n","\n","def cut_negative_boundary(box):\n","    res = []\n","    for x in box['box']:\n","        if x < 0:\n","            x = 0\n","        res.append(x)\n","    box['box'] = res\n","    return box\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GaJuavSBgGTM"},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","def preprocess_image(image_path):\n","    # Load the image using OpenCV\n","    image = cv2.imread(image_path)    \n","    \n","    # Resize the cropped face image to a consistent size (e.g., 224x224 pixels)\n","    resized_image = cv2.resize(image, (224, 224))\n","    \n","    # Convert the image to grayscale or RGB based on the model's input requirements\n","    rgb_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n","    \n","    # Normalize the pixel values to be in the range [0, 1]\n","    normalized_image = rgb_image / 255.0\n","    \n","    # Prepare the input data as a numpy array or tensor\n","    preprocessed_image = np.expand_dims(normalized_image, axis=0)\n","    \n","    return preprocessed_image\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yVvYAzEPgYsX"},"outputs":[],"source":["train.head()\n","train_processed_dir = path+'Train/train_aligned/'\n","test_processed_dir = path+'Test/test_aligned/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x5QaROhFgQZ8"},"outputs":[],"source":["import pandas as pd\n","import os\n","import numpy as np\n","\n","\n","# Prepare train data\n","train_images = []\n","train_bmi_labels = []\n","train_gender_labels = []\n","\n","for index, row in train.iterrows():\n","    #image_path = os.path.join(path+'Train/train_aligned/', row['name'])\n","    #preprocessed_image = preprocess_image(image_path)\n","    \n","    #train_images.append(preprocessed_image)\n","    train_bmi_labels.append(row['bmi'])\n","    train_gender_labels.append(row['gender'])\n","\n","#train_images = np.array(train_images)\n","train_bmi_labels = np.array(train_bmi_labels)\n","train_gender_labels = np.array(train_gender_labels)\n","\n","# Prepare test data\n","test_images = []\n","test_bmi_labels = []\n","test_gender_labels = []\n","\n","for index, row in valid.iterrows():\n","    #image_path = os.path.join(path+'Test/test_aligned/', row['name'])\n","    #preprocessed_image = preprocess_image(image_path)\n","    \n","    #test_images.append(preprocessed_image)\n","    test_bmi_labels.append(row['bmi'])\n","    test_gender_labels.append(row['gender'])\n","\n","#test_images = np.array(test_images)\n","test_bmi_labels = np.array(test_bmi_labels)\n","test_gender_labels = np.array(test_gender_labels)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":145,"status":"ok","timestamp":1684291237359,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"UXUtHQNMhBHy","outputId":"56460c9c-cd14-459c-abe4-6c586f196e32"},"outputs":[{"data":{"text/plain":["(750, 224, 224, 3)"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["test_images.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":142,"status":"ok","timestamp":1684291195352,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"VfXTGwjTjlpP","outputId":"2cdb6712-fb14-4e06-c6a1-868d4987cfe5"},"outputs":[{"data":{"text/plain":["(3203,)"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["train_bmi_labels.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M_6Hezh-jqK1"},"outputs":[],"source":["train_images = np.squeeze(train_images, axis=1)\n","test_images = np.squeeze(test_images, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7hocyANhvLC_"},"outputs":[],"source":["# Save the train_images array to a file\n","np.save(path+'train_images.npy', train_images)\n","np.save(path+'test_images.npy', test_images)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8fBN-JqbvU36"},"outputs":[],"source":["# Load the saved train_images array\n","train_images = np.load(path+'train_images.npy')\n","test_images = np.load(path+'test_images.npy')\n"]},{"cell_type":"markdown","metadata":{"id":"BDYDzrTz2WZA"},"source":["### Model 5 : CNN model to predict BMI and Gender"]},{"cell_type":"markdown","metadata":{"id":"_LraKMg2ibVw"},"source":["The purpose of training the model using gender information as an additional input and predicting gender again as one of the outputs is to create a multi-task learning setup. In this scenario, the model is trained not only to predict BMI but also to predict gender simultaneously."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HhNtGUDMH5pj"},"outputs":[],"source":["from keras.layers import Concatenate\n","\n","input_image = tf.keras.layers.Input(shape=(224, 224, 3), name='input_image')\n","input_gender = tf.keras.layers.Input(shape=(1,), name='input_gender')\n","\n","conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu')(input_image)\n","pool1 = tf.keras.layers.MaxPooling2D(pool_size=2)(conv1)\n","\n","conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu')(pool1)\n","pool2 = tf.keras.layers.MaxPooling2D(pool_size=2)(conv2)\n","\n","conv3 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu')(pool2)\n","pool3 = tf.keras.layers.MaxPooling2D(pool_size=2)(conv3)\n","\n","flatten = tf.keras.layers.Flatten()(pool3)\n","\n","# Gender branch\n","dense_gender = tf.keras.layers.Dense(units=32, activation='relu')(input_gender)\n","\n","# Concatenate the image and gender features\n","concatenated = tf.keras.layers.Concatenate()([flatten, dense_gender])\n","\n","# Output layers\n","dense1 = tf.keras.layers.Dense(units=32, activation='relu')(concatenated)\n","output_BMI = tf.keras.layers.Dense(units=1, activation='relu', name='output_BMI')(dense1)\n","output_gender = tf.keras.layers.Dense(units=1, activation='sigmoid', name='output_gender')(dense1)\n","\n","# Create the model\n","model_BMI_gender_cnn = tf.keras.Model(inputs=[input_image, input_gender], outputs=[output_BMI, output_gender])\n","\n","# Compile the model\n","model_BMI_gender_cnn.compile(loss=['mse', 'binary_crossentropy'], optimizer='adam', metrics=['mae', 'accuracy'])\n","\n","# Fit the model to the training data\n","model_BMI_gender_cnn.fit([train_images, train_gender_labels], [np.log(train_bmi_labels), train_gender_labels], epochs=13, batch_size=1)\n"]},{"cell_type":"markdown","metadata":{"id":"CTrf_DMGKx6H"},"source":["Epoch 1/13\n","3203/3203 [==============================] - 487s 151ms/step - loss: 12.0406 - output_BMI_loss: 11.9630 - output_gender_loss: 0.0776 - output_BMI_mae: 3.4508 - output_BMI_accuracy: 0.0000e+00 - output_gender_mae: 0.0530 - output_gender_accuracy: 0.9819\n","\n","Epoch 2/13\n","3203/3203 [==============================] - 487s 152ms/step - loss: 11.9631 - output_BMI_loss: 11.9629 - output_gender_loss: 1.4063e-04 - output_BMI_mae: 3.4508 - output_BMI_accuracy: 0.0000e+00 - output_gender_mae: 1.4061e-04 - output_gender_accuracy: 1.0000\n","\n","Epoch 3/13\n","3203/3203 [==============================] - 486s 152ms/step - loss: 11.9630 - output_BMI_loss: 11.9630 - output_gender_loss: 1.6620e-05 - output_BMI_mae: 3.4508 - output_BMI_accuracy: 0.0000e+00 - output_gender_mae: 1.6619e-05 - output_gender_accuracy: 1.0000\n","\n","Epoch 4/13\n","3203/3203 [==============================] - 483s 151ms/step - loss: 11.9630 - output_BMI_loss: 11.9630 - output_gender_loss: 2.5776e-06 - output_BMI_mae: 3.4508 - output_BMI_accuracy: 0.0000e+00 - output_gender_mae: 2.5771e-06 - output_gender_accuracy: 1.0000\n","\n","Epoch 5/13\n","3203/3203 [==============================] - 483s 151ms/step - loss: 11.9630 - output_BMI_loss: 11.9630 - output_gender_loss: 4.2830e-07 - output_BMI_mae: 3.4508 - output_BMI_accuracy: 0.0000e+00 - output_gender_mae: 4.2818e-07 - output_gender_accuracy: 1.0000\n","\n","Epoch 6/13\n","3203/3203 [==============================] - 482s 151ms/step - loss: 11.9630 - output_BMI_loss: 11.9630 - output_gender_loss: 7.3262e-08 - output_BMI_mae: 3.4508 - output_BMI_accuracy: 0.0000e+00 - output_gender_mae: 7.1090e-08 - output_gender_accuracy: 1.0000\n","\n","Epoch 7/13\n","3203/3203 [==============================] - 483s 151ms/step - loss: 11.9629 - output_BMI_loss: 11.9629 - output_gender_loss: 1.5085e-08 - output_BMI_mae: 3.4508 - output_BMI_accuracy: 0.0000e+00 - output_gender_mae: 1.1093e-08 - output_gender_accuracy: 1.0000\n","\n","Epoch 8/13\n","3203/3203 [==============================] - 482s 151ms/step - loss: 11.9630 - output_BMI_loss: 11.9630 - output_gender_loss: 5.6973e-09 - output_BMI_mae: 3.4508 - output_BMI_accuracy: 0.0000e+00 - output_gender_mae: 1.9156e-09 - output_gender_accuracy: 1.0000\n","\n","Epoch 9/13\n","3203/3203 [==============================] - 482s 150ms/step - loss: 11.9630 - output_BMI_loss: 11.9630 - output_gender_loss: 3.2846e-09 - output_BMI_mae: 3.4508 - output_BMI_accuracy: 0.0000e+00 - output_gender_mae: 4.7337e-10 - output_gender_accuracy: 1.0000\n","\n","Epoch 10/13\n","3203/3203 [==============================] - 482s 150ms/step - loss: 11.9629 - output_BMI_loss: 11.9629 - output_gender_loss: 3.0057e-09 - output_BMI_mae: 3.4508 - output_BMI_accuracy: 0.0000e+00 - output_gender_mae: 1.6060e-10 - output_gender_accuracy: 1.0000\n","\n","Epoch 11/13\n","3203/3203 [==============================] - 482s 150ms/step - loss: 11.9629 - output_BMI_loss: 11.9629 - output_gender_loss: 2.3996e-09 - output_BMI_mae: 3.4508 - output_BMI_accuracy: 0.0000e+00 - output_gender_mae: 7.5919e-11 - output_gender_accuracy: 1.0000\n","\n","Epoch 12/13\n","3203/3203 [==============================] - 481s 150ms/step - loss: 11.9629 - output_BMI_loss: 11.9629 - output_gender_loss: 3.1511e-09 - output_BMI_mae: 3.4508 - output_BMI_accuracy: 0.0000e+00 - output_gender_mae: 3.8642e-11 - output_gender_accuracy: 1.0000\n","\n","Epoch 13/13\n","3203/3203 [==============================] - 481s 150ms/step - loss: 11.9629 - output_BMI_loss: 11.9629 - output_gender_loss: 2.4407e-09 - output_BMI_mae: 3.4508 - output_BMI_accuracy: 0.0000e+00 - output_gender_mae: 1.9102e-11 - output_gender_accuracy: 1.0000\n","\n","24/24 [==============================] - 30s 1s/step - loss: 12.1983 - output_BMI_loss: 12.1983 - output_gender_loss: 5.0526e-09 - output_BMI_mae: 3.4833 - output_BMI_accuracy: 0.0000e+00 - output_gender_mae: 1.9673e-12 - output_gender_accuracy: 1.0000\n"]},{"cell_type":"markdown","metadata":{"id":"b1_zrxAE2WZB"},"source":["#### Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42475,"status":"ok","timestamp":1684301195277,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"ofyNE5IBKua7","outputId":"d22208de-38b9-49a9-c874-8be87c3279db"},"outputs":[{"name":"stdout","output_type":"stream","text":["24/24 [==============================] - 30s 1s/step - loss: 12.1983 - output_BMI_loss: 12.1983 - output_gender_loss: 5.0526e-09 - output_BMI_mae: 3.4833 - output_BMI_accuracy: 0.0000e+00 - output_gender_mae: 1.9673e-12 - output_gender_accuracy: 1.0000\n"]}],"source":["# Evaluate the model on the test data\n","predictions = model_BMI_gender_cnn.evaluate([test_images, test_gender_labels], [np.log(test_bmi_labels), test_gender_labels])\n","\n","model_BMI_gender_cnn.save(path+'saved_model/cnn_BMI_gender.h5')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":118,"status":"ok","timestamp":1684301619102,"user":{"displayName":"Swathi Ganesan","userId":"16038909399462937339"},"user_tz":300},"id":"Bf4UoRsClnlD","outputId":"8e6c4f69-a2ea-437b-83dc-0f4883964fcf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss (BMI): 12.198328018188477\n","MAE (BMI): 3.483299493789673\n","Loss (Gender): 5.052609441946743e-09\n","Accuracy (Gender): 1.0\n"]}],"source":["test_loss = predictions[0]\n","gender_loss = predictions[2]\n","test_BMI_mae = predictions[3]\n","test_gender_accuracy = predictions[-1]\n","\n","# Print the results\n","print('Loss (BMI):', test_loss)\n","print('MAE (BMI):', test_BMI_mae)\n","print('Loss (Gender):', gender_loss)\n","print('Accuracy (Gender):', test_gender_accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVkeWzhdMPye"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}