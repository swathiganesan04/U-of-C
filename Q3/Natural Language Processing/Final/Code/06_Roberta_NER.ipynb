{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26e305af-c39d-4bf6-b319-53e97ec837e9",
   "metadata": {},
   "source": [
    "### Top level NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c9ce65-bb22-4e71-aebe-7182cd6a2b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 01:02:16.570323: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-26 01:02:19.810276: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-26 01:02:19.810410: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-26 01:02:19.810422: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-05-26 01:02:22.387414: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-26 01:02:22.387462: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-05-26 01:02:22.387490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tensorflow-nlp): /proc/driver/nvidia/version does not exist\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import sys\n",
    "import regex as re\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Using Spacy Large\n",
    "#!python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=[\"tagger\", \"parser\"])\n",
    "\n",
    "# Enable only NER\n",
    "nlp.enable_pipe(\"ner\")\n",
    "\n",
    "import nltk as nltk\n",
    "import nltk.corpus  \n",
    "from nltk.text import Text\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# Graphics in SVG format are more sharp and legible\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c15df2-371c-4c39-9d1e-bd952b017f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPUs: 16\n",
      "INFO: Pandarallel will run on 15 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#!pip install pandarallel\n",
    "import multiprocessing\n",
    "\n",
    "num_processors = multiprocessing.cpu_count()\n",
    "print(f'Available CPUs: {num_processors}')\n",
    "\n",
    "import pandarallel\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=num_processors-1, use_memory_fs=False, progress_bar=True)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f823eb11-c412-416c-8616-d80658e106e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_ner(x):\n",
    "    doc = nlp(x)\n",
    "    entities = []\n",
    "    labels = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append(ent.text)\n",
    "        labels.append(ent.label_)\n",
    "    entities_labels = list(zip(entities, labels)) #we do not want unique entities\n",
    "    return entities_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24939894-0ccb-49d3-91de-be90cb4f745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be4d6e6-7f81-4a48-8be9-6517505386ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ec1568c-7558-4003-9e82-a53ffc9c18d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 15s, sys: 17 s, total: 1min 32s\n",
      "Wall time: 1min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(198564, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_processed = pd.read_parquet('06_sent_trans_df.parquet', engine='pyarrow')\n",
    "df_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bef6663-41ee-4bed-94eb-deb8130de454",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the 'year_month' column to datetime format\n",
    "df_processed['formatted_date'] = pd.to_datetime(df_processed['year_month'].astype(str) + '-01').dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# Use the 'dt' accessor to extract the month and year, and format it as \"Jan 2019\"\n",
    "df_processed['year_month'] = df_processed['year_month'].dt.strftime('%b %Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af2d47af-c711-462b-acfe-a2e0412c6bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>year_month</th>\n",
       "      <th>clean_text_case</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text_sent</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>formatted_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156465</th>\n",
       "      <td>2023-01-07</td>\n",
       "      <td>Citizen's New Watch Uses NASA Tech, AI To Meas...</td>\n",
       "      <td>Citizen's New Watch Uses NASA Tech, AI To Meas...</td>\n",
       "      <td>Jan 2023</td>\n",
       "      <td>[Citizen, New, Watch, Uses, NASA, Tech, AI, Me...</td>\n",
       "      <td>[citizen, new, watch, uses, nasa, tech, ai, me...</td>\n",
       "      <td>citizen new watch us nasa tech ai measure fatigue</td>\n",
       "      <td>citizen new watch uses nasa tech ai measure fa...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.995664</td>\n",
       "      <td>2023-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107237</th>\n",
       "      <td>2023-01-19</td>\n",
       "      <td>aiOla raises $25 million to interface natural ...</td>\n",
       "      <td>aiOla raises $25 million to interface natural ...</td>\n",
       "      <td>Jan 2023</td>\n",
       "      <td>[aiOla, raise, million, interface, natural, la...</td>\n",
       "      <td>[aiola, raise, million, interface, natural, la...</td>\n",
       "      <td>aiola raise 25 million interface natural langu...</td>\n",
       "      <td>aiola raise million interface natural language...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>2023-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67533</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>Attention raises $3.1M, unveiling their AI sal...</td>\n",
       "      <td>Attention raises $3.1M, unveiling their AI sal...</td>\n",
       "      <td>Jan 2023</td>\n",
       "      <td>[Attention, raise, 31M, unveiling, AI, sale, a...</td>\n",
       "      <td>[attention, raise, 31m, unveiling, ai, sale, a...</td>\n",
       "      <td>attention raise 31m unveiling ai sale assistant</td>\n",
       "      <td>attention raise 31m unveiling ai sale assistan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.999822</td>\n",
       "      <td>2023-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date                                              title  \\\n",
       "156465 2023-01-07  Citizen's New Watch Uses NASA Tech, AI To Meas...   \n",
       "107237 2023-01-19  aiOla raises $25 million to interface natural ...   \n",
       "67533  2023-01-10  Attention raises $3.1M, unveiling their AI sal...   \n",
       "\n",
       "                                                     text year_month  \\\n",
       "156465  Citizen's New Watch Uses NASA Tech, AI To Meas...   Jan 2023   \n",
       "107237  aiOla raises $25 million to interface natural ...   Jan 2023   \n",
       "67533   Attention raises $3.1M, unveiling their AI sal...   Jan 2023   \n",
       "\n",
       "                                          clean_text_case  \\\n",
       "156465  [Citizen, New, Watch, Uses, NASA, Tech, AI, Me...   \n",
       "107237  [aiOla, raise, million, interface, natural, la...   \n",
       "67533   [Attention, raise, 31M, unveiling, AI, sale, a...   \n",
       "\n",
       "                                               clean_text  \\\n",
       "156465  [citizen, new, watch, uses, nasa, tech, ai, me...   \n",
       "107237  [aiola, raise, million, interface, natural, la...   \n",
       "67533   [attention, raise, 31m, unveiling, ai, sale, a...   \n",
       "\n",
       "                                              clean_title  \\\n",
       "156465  citizen new watch us nasa tech ai measure fatigue   \n",
       "107237  aiola raise 25 million interface natural langu...   \n",
       "67533     attention raise 31m unveiling ai sale assistant   \n",
       "\n",
       "                                          clean_text_sent sentiment_label  \\\n",
       "156465  citizen new watch uses nasa tech ai measure fa...         neutral   \n",
       "107237  aiola raise million interface natural language...         neutral   \n",
       "67533   attention raise 31m unveiling ai sale assistan...         neutral   \n",
       "\n",
       "        sentiment_score formatted_date  \n",
       "156465         0.995664     2023-01-01  \n",
       "107237         0.999852     2023-01-01  \n",
       "67533          0.999822     2023-01-01  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c99f5167-eccf-4f27-bc19-59a87fa2810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = df_processed[df_processed['sentiment_label'] == 'positive']\n",
    "df_neg = df_processed[df_processed['sentiment_label'] == 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63304b38-919e-48b2-9512-48df551d21da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4410"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1374b48c-e07b-43ee-af2d-b71b34dab7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75ad17acc244e338d35fc4342653599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=13238), Label(value='0 / 13238')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.3 s, sys: 7.69 s, total: 27 s\n",
      "Wall time: 25.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_processed['clean_text_case_sent'] = df_processed['clean_text_case'].parallel_apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c262502f-89c2-4aed-9173-3748080321d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da9cc2123694b899e07671b85b8bd14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=13238), Label(value='0 / 13238')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198564/198564 [00:00<00:00, 928859.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 2s, sys: 21.8 s, total: 2min 24s\n",
      "Wall time: 28min 39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tqdm.pandas()\n",
    "df_processed['entities_spacy'] = df_processed['clean_text_case_sent'].parallel_apply(lambda x: spacy_ner(x)).progress_apply(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e24ec193-88e7-4214-a99b-eecc47b1b44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>year_month</th>\n",
       "      <th>clean_text_case</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text_sent</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>formatted_date</th>\n",
       "      <th>clean_text_case_sent</th>\n",
       "      <th>entities_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-18</td>\n",
       "      <td>Artificial intelligence improves parking effic...</td>\n",
       "      <td>\\n\\nArtificial intelligence improves parking e...</td>\n",
       "      <td>Mar 2021</td>\n",
       "      <td>[Artificial, intelligence, improves, parking, ...</td>\n",
       "      <td>[artificial, intelligence, improves, parking, ...</td>\n",
       "      <td>artificial intelligence improves parking effic...</td>\n",
       "      <td>artificial intelligence improves parking effic...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.997408</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>Artificial intelligence improves parking effic...</td>\n",
       "      <td>[(Chinese, NORP), (Chinese, NORP), (Japanese, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>Children With Autism Saw Their Learning and So...</td>\n",
       "      <td>\\nChildren With Autism Saw Their Learning and ...</td>\n",
       "      <td>Feb 2020</td>\n",
       "      <td>[Children, Autism, Saw, Learning, Social, Skil...</td>\n",
       "      <td>[children, autism, saw, learning, social, skil...</td>\n",
       "      <td>child autism saw learning social skill boosted...</td>\n",
       "      <td>children autism saw learning social skills boo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.999264</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>Children Autism Saw Learning Social Skills Boo...</td>\n",
       "      <td>[(Thursday, DATE), (Drax, ORG), (March, DATE),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-26</td>\n",
       "      <td>Forget ML, AI and Industry 4.0 – obsolescence ...</td>\n",
       "      <td>\\n\\nForget ML, AI and Industry 4.0 – obsolesce...</td>\n",
       "      <td>Mar 2021</td>\n",
       "      <td>[Forget, ML, AI, Industry, 40, obsolescence, f...</td>\n",
       "      <td>[forget, ml, ai, industry, 40, obsolescence, f...</td>\n",
       "      <td>forget ml ai industry 40 – obsolescence focus ...</td>\n",
       "      <td>forget ml ai industry 40 obsolescence focus fe...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.999864</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>Forget ML AI Industry 40 obsolescence focus Fe...</td>\n",
       "      <td>[(Forget ML AI Industry, ORG), (40, CARDINAL),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-10</td>\n",
       "      <td>Strategy Analytics: 71% of Smartphones Sold Gl...</td>\n",
       "      <td>\\n\\nStrategy Analytics: 71% of Smartphones Sol...</td>\n",
       "      <td>Mar 2021</td>\n",
       "      <td>[Strategy, Analytics, Smartphones, Sold, Globa...</td>\n",
       "      <td>[strategy, analytics, smartphones, sold, globa...</td>\n",
       "      <td>strategy analytics 71 smartphones sold globall...</td>\n",
       "      <td>strategy analytics smartphones sold globally a...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.999874</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>Strategy Analytics Smartphones Sold Globally A...</td>\n",
       "      <td>[(AI Powered hour ago, TIME), (Artificial Inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-10-20</td>\n",
       "      <td>Olympus to Support Endoscopic AI Diagnosis Edu...</td>\n",
       "      <td>\\n\\nOlympus to Support Endoscopic AI Diagnosis...</td>\n",
       "      <td>Oct 2020</td>\n",
       "      <td>[Olympus, Support, Endoscopic, AI, Diagnosis, ...</td>\n",
       "      <td>[olympus, support, endoscopic, ai, diagnosis, ...</td>\n",
       "      <td>olympus support endoscopic ai diagnosis educat...</td>\n",
       "      <td>olympus support endoscopic ai diagnosis educat...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.999858</td>\n",
       "      <td>2020-10-01</td>\n",
       "      <td>Olympus Support Endoscopic AI Diagnosis Educat...</td>\n",
       "      <td>[(Olympus Support Endoscopic AI Diagnosis Educ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                              title  \\\n",
       "0 2021-03-18  Artificial intelligence improves parking effic...   \n",
       "1 2020-02-27  Children With Autism Saw Their Learning and So...   \n",
       "2 2021-03-26  Forget ML, AI and Industry 4.0 – obsolescence ...   \n",
       "3 2021-03-10  Strategy Analytics: 71% of Smartphones Sold Gl...   \n",
       "4 2020-10-20  Olympus to Support Endoscopic AI Diagnosis Edu...   \n",
       "\n",
       "                                                text year_month  \\\n",
       "0  \\n\\nArtificial intelligence improves parking e...   Mar 2021   \n",
       "1  \\nChildren With Autism Saw Their Learning and ...   Feb 2020   \n",
       "2  \\n\\nForget ML, AI and Industry 4.0 – obsolesce...   Mar 2021   \n",
       "3  \\n\\nStrategy Analytics: 71% of Smartphones Sol...   Mar 2021   \n",
       "4  \\n\\nOlympus to Support Endoscopic AI Diagnosis...   Oct 2020   \n",
       "\n",
       "                                     clean_text_case  \\\n",
       "0  [Artificial, intelligence, improves, parking, ...   \n",
       "1  [Children, Autism, Saw, Learning, Social, Skil...   \n",
       "2  [Forget, ML, AI, Industry, 40, obsolescence, f...   \n",
       "3  [Strategy, Analytics, Smartphones, Sold, Globa...   \n",
       "4  [Olympus, Support, Endoscopic, AI, Diagnosis, ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  [artificial, intelligence, improves, parking, ...   \n",
       "1  [children, autism, saw, learning, social, skil...   \n",
       "2  [forget, ml, ai, industry, 40, obsolescence, f...   \n",
       "3  [strategy, analytics, smartphones, sold, globa...   \n",
       "4  [olympus, support, endoscopic, ai, diagnosis, ...   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0  artificial intelligence improves parking effic...   \n",
       "1  child autism saw learning social skill boosted...   \n",
       "2  forget ml ai industry 40 – obsolescence focus ...   \n",
       "3  strategy analytics 71 smartphones sold globall...   \n",
       "4  olympus support endoscopic ai diagnosis educat...   \n",
       "\n",
       "                                     clean_text_sent sentiment_label  \\\n",
       "0  artificial intelligence improves parking effic...        positive   \n",
       "1  children autism saw learning social skills boo...        positive   \n",
       "2  forget ml ai industry 40 obsolescence focus fe...         neutral   \n",
       "3  strategy analytics smartphones sold globally a...         neutral   \n",
       "4  olympus support endoscopic ai diagnosis educat...         neutral   \n",
       "\n",
       "   sentiment_score formatted_date  \\\n",
       "0         0.997408     2021-03-01   \n",
       "1         0.999264     2020-02-01   \n",
       "2         0.999864     2021-03-01   \n",
       "3         0.999874     2021-03-01   \n",
       "4         0.999858     2020-10-01   \n",
       "\n",
       "                                clean_text_case_sent  \\\n",
       "0  Artificial intelligence improves parking effic...   \n",
       "1  Children Autism Saw Learning Social Skills Boo...   \n",
       "2  Forget ML AI Industry 40 obsolescence focus Fe...   \n",
       "3  Strategy Analytics Smartphones Sold Globally A...   \n",
       "4  Olympus Support Endoscopic AI Diagnosis Educat...   \n",
       "\n",
       "                                      entities_spacy  \n",
       "0  [(Chinese, NORP), (Chinese, NORP), (Japanese, ...  \n",
       "1  [(Thursday, DATE), (Drax, ORG), (March, DATE),...  \n",
       "2  [(Forget ML AI Industry, ORG), (40, CARDINAL),...  \n",
       "3  [(AI Powered hour ago, TIME), (Artificial Inte...  \n",
       "4  [(Olympus Support Endoscopic AI Diagnosis Educ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45c37094-31ff-4be9-a310-f65e05cc8d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51284e9f0c7241d69fee794cd490f9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=13238), Label(value='0 / 13238')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36139c2d21984be4976881277824c75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=13238), Label(value='0 / 13238')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5687bd90f84cb891113c5d0cefb6af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=13238), Label(value='0 / 13238')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c2fbd60d3c469096b861cb757e164c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=13238), Label(value='0 / 13238')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_processed['ENT_ORG'] = df_processed['entities_spacy'].parallel_apply(lambda x: [tup[0] for tup in x if tup[1] == \"ORG\"])\n",
    "df_processed['ENT_PROD'] = df_processed['entities_spacy'].parallel_apply(lambda x: [tup[0] for tup in x if tup[1] == \"PRODUCT\"])\n",
    "df_processed['ENT_PER'] = df_processed['entities_spacy'].parallel_apply(lambda x: [tup[0] for tup in x if tup[1] == \"PERSON\"])\n",
    "df_processed['ENT_NORP'] = df_processed['entities_spacy'].parallel_apply(lambda x: [tup[0] for tup in x if tup[1] == \"COMPANY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37622b3c-f360-4e73-92bf-f6c816acab31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>year_month</th>\n",
       "      <th>clean_text_case</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text_sent</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>formatted_date</th>\n",
       "      <th>clean_text_case_sent</th>\n",
       "      <th>entities_spacy</th>\n",
       "      <th>ENT_ORG</th>\n",
       "      <th>ENT_PROD</th>\n",
       "      <th>ENT_PER</th>\n",
       "      <th>ENT_NORP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-18</td>\n",
       "      <td>Artificial intelligence improves parking effic...</td>\n",
       "      <td>\\n\\nArtificial intelligence improves parking e...</td>\n",
       "      <td>Mar 2021</td>\n",
       "      <td>[Artificial, intelligence, improves, parking, ...</td>\n",
       "      <td>[artificial, intelligence, improves, parking, ...</td>\n",
       "      <td>artificial intelligence improves parking effic...</td>\n",
       "      <td>artificial intelligence improves parking effic...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.997408</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>Artificial intelligence improves parking effic...</td>\n",
       "      <td>[(Chinese, NORP), (Chinese, NORP), (Japanese, ...</td>\n",
       "      <td>[ETC, ETC, ETC, ETC, ETC, Wang, ETC, AIpark Sk...</td>\n",
       "      <td>[AI, AI, AI, AI, AI]</td>\n",
       "      <td>[Wang life, AIpark Sky Eye, Xiang Yanping, AIp...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>Children With Autism Saw Their Learning and So...</td>\n",
       "      <td>\\nChildren With Autism Saw Their Learning and ...</td>\n",
       "      <td>Feb 2020</td>\n",
       "      <td>[Children, Autism, Saw, Learning, Social, Skil...</td>\n",
       "      <td>[children, autism, saw, learning, social, skil...</td>\n",
       "      <td>child autism saw learning social skill boosted...</td>\n",
       "      <td>children autism saw learning social skills boo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.999264</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>Children Autism Saw Learning Social Skills Boo...</td>\n",
       "      <td>[(Thursday, DATE), (Drax, ORG), (March, DATE),...</td>\n",
       "      <td>[Drax, Italy Coronavirus Dettol, News Parliame...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Levi Strauss, Mansplaining, Levi Strauss, Sho...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-26</td>\n",
       "      <td>Forget ML, AI and Industry 4.0 – obsolescence ...</td>\n",
       "      <td>\\n\\nForget ML, AI and Industry 4.0 – obsolesce...</td>\n",
       "      <td>Mar 2021</td>\n",
       "      <td>[Forget, ML, AI, Industry, 40, obsolescence, f...</td>\n",
       "      <td>[forget, ml, ai, industry, 40, obsolescence, f...</td>\n",
       "      <td>forget ml ai industry 40 – obsolescence focus ...</td>\n",
       "      <td>forget ml ai industry 40 obsolescence focus fe...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.999864</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>Forget ML AI Industry 40 obsolescence focus Fe...</td>\n",
       "      <td>[(Forget ML AI Industry, ORG), (40, CARDINAL),...</td>\n",
       "      <td>[Forget ML AI Industry, Signal LSI Circuit Sys...</td>\n",
       "      <td>[Britishmade, Throughhole, lowtemp]</td>\n",
       "      <td>[Willian Santos, Durafuse LT]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-10</td>\n",
       "      <td>Strategy Analytics: 71% of Smartphones Sold Gl...</td>\n",
       "      <td>\\n\\nStrategy Analytics: 71% of Smartphones Sol...</td>\n",
       "      <td>Mar 2021</td>\n",
       "      <td>[Strategy, Analytics, Smartphones, Sold, Globa...</td>\n",
       "      <td>[strategy, analytics, smartphones, sold, globa...</td>\n",
       "      <td>strategy analytics 71 smartphones sold globall...</td>\n",
       "      <td>strategy analytics smartphones sold globally a...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.999874</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>Strategy Analytics Smartphones Sold Globally A...</td>\n",
       "      <td>[(AI Powered hour ago, TIME), (Artificial Inte...</td>\n",
       "      <td>[Artificial Intelligence Powers, Smartphones B...</td>\n",
       "      <td>[AI, ondevice AI, AI, AI, AI, AI, AI, AI, Pro ...</td>\n",
       "      <td>[Ken Hyers, Ukonaho, Ken Hyers, Ukonaho Camera...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-10-20</td>\n",
       "      <td>Olympus to Support Endoscopic AI Diagnosis Edu...</td>\n",
       "      <td>\\n\\nOlympus to Support Endoscopic AI Diagnosis...</td>\n",
       "      <td>Oct 2020</td>\n",
       "      <td>[Olympus, Support, Endoscopic, AI, Diagnosis, ...</td>\n",
       "      <td>[olympus, support, endoscopic, ai, diagnosis, ...</td>\n",
       "      <td>olympus support endoscopic ai diagnosis educat...</td>\n",
       "      <td>olympus support endoscopic ai diagnosis educat...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.999858</td>\n",
       "      <td>2020-10-01</td>\n",
       "      <td>Olympus Support Endoscopic AI Diagnosis Educat...</td>\n",
       "      <td>[(Olympus Support Endoscopic AI Diagnosis Educ...</td>\n",
       "      <td>[Olympus Support Endoscopic AI Diagnosis Educa...</td>\n",
       "      <td>[AI, AI, AI]</td>\n",
       "      <td>[Satoshi Hemmi, Yuka Horimoto819024901071]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                              title  \\\n",
       "0 2021-03-18  Artificial intelligence improves parking effic...   \n",
       "1 2020-02-27  Children With Autism Saw Their Learning and So...   \n",
       "2 2021-03-26  Forget ML, AI and Industry 4.0 – obsolescence ...   \n",
       "3 2021-03-10  Strategy Analytics: 71% of Smartphones Sold Gl...   \n",
       "4 2020-10-20  Olympus to Support Endoscopic AI Diagnosis Edu...   \n",
       "\n",
       "                                                text year_month  \\\n",
       "0  \\n\\nArtificial intelligence improves parking e...   Mar 2021   \n",
       "1  \\nChildren With Autism Saw Their Learning and ...   Feb 2020   \n",
       "2  \\n\\nForget ML, AI and Industry 4.0 – obsolesce...   Mar 2021   \n",
       "3  \\n\\nStrategy Analytics: 71% of Smartphones Sol...   Mar 2021   \n",
       "4  \\n\\nOlympus to Support Endoscopic AI Diagnosis...   Oct 2020   \n",
       "\n",
       "                                     clean_text_case  \\\n",
       "0  [Artificial, intelligence, improves, parking, ...   \n",
       "1  [Children, Autism, Saw, Learning, Social, Skil...   \n",
       "2  [Forget, ML, AI, Industry, 40, obsolescence, f...   \n",
       "3  [Strategy, Analytics, Smartphones, Sold, Globa...   \n",
       "4  [Olympus, Support, Endoscopic, AI, Diagnosis, ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  [artificial, intelligence, improves, parking, ...   \n",
       "1  [children, autism, saw, learning, social, skil...   \n",
       "2  [forget, ml, ai, industry, 40, obsolescence, f...   \n",
       "3  [strategy, analytics, smartphones, sold, globa...   \n",
       "4  [olympus, support, endoscopic, ai, diagnosis, ...   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0  artificial intelligence improves parking effic...   \n",
       "1  child autism saw learning social skill boosted...   \n",
       "2  forget ml ai industry 40 – obsolescence focus ...   \n",
       "3  strategy analytics 71 smartphones sold globall...   \n",
       "4  olympus support endoscopic ai diagnosis educat...   \n",
       "\n",
       "                                     clean_text_sent sentiment_label  \\\n",
       "0  artificial intelligence improves parking effic...        positive   \n",
       "1  children autism saw learning social skills boo...        positive   \n",
       "2  forget ml ai industry 40 obsolescence focus fe...         neutral   \n",
       "3  strategy analytics smartphones sold globally a...         neutral   \n",
       "4  olympus support endoscopic ai diagnosis educat...         neutral   \n",
       "\n",
       "   sentiment_score formatted_date  \\\n",
       "0         0.997408     2021-03-01   \n",
       "1         0.999264     2020-02-01   \n",
       "2         0.999864     2021-03-01   \n",
       "3         0.999874     2021-03-01   \n",
       "4         0.999858     2020-10-01   \n",
       "\n",
       "                                clean_text_case_sent  \\\n",
       "0  Artificial intelligence improves parking effic...   \n",
       "1  Children Autism Saw Learning Social Skills Boo...   \n",
       "2  Forget ML AI Industry 40 obsolescence focus Fe...   \n",
       "3  Strategy Analytics Smartphones Sold Globally A...   \n",
       "4  Olympus Support Endoscopic AI Diagnosis Educat...   \n",
       "\n",
       "                                      entities_spacy  \\\n",
       "0  [(Chinese, NORP), (Chinese, NORP), (Japanese, ...   \n",
       "1  [(Thursday, DATE), (Drax, ORG), (March, DATE),...   \n",
       "2  [(Forget ML AI Industry, ORG), (40, CARDINAL),...   \n",
       "3  [(AI Powered hour ago, TIME), (Artificial Inte...   \n",
       "4  [(Olympus Support Endoscopic AI Diagnosis Educ...   \n",
       "\n",
       "                                             ENT_ORG  \\\n",
       "0  [ETC, ETC, ETC, ETC, ETC, Wang, ETC, AIpark Sk...   \n",
       "1  [Drax, Italy Coronavirus Dettol, News Parliame...   \n",
       "2  [Forget ML AI Industry, Signal LSI Circuit Sys...   \n",
       "3  [Artificial Intelligence Powers, Smartphones B...   \n",
       "4  [Olympus Support Endoscopic AI Diagnosis Educa...   \n",
       "\n",
       "                                            ENT_PROD  \\\n",
       "0                               [AI, AI, AI, AI, AI]   \n",
       "1                                                 []   \n",
       "2                [Britishmade, Throughhole, lowtemp]   \n",
       "3  [AI, ondevice AI, AI, AI, AI, AI, AI, AI, Pro ...   \n",
       "4                                       [AI, AI, AI]   \n",
       "\n",
       "                                             ENT_PER ENT_NORP  \n",
       "0  [Wang life, AIpark Sky Eye, Xiang Yanping, AIp...       []  \n",
       "1  [Levi Strauss, Mansplaining, Levi Strauss, Sho...       []  \n",
       "2                      [Willian Santos, Durafuse LT]       []  \n",
       "3  [Ken Hyers, Ukonaho, Ken Hyers, Ukonaho Camera...       []  \n",
       "4         [Satoshi Hemmi, Yuka Horimoto819024901071]       []  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5037a45-4547-4776-9f86-a93d3320dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.to_parquet('trans_sent_ner.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b06b2b-4ec8-45e3-84ee-d68b626347fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 58s, sys: 30 s, total: 2min 28s\n",
      "Wall time: 2min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(198564, 17)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_ner = pd.read_parquet('trans_sent_ner.parquet', engine='pyarrow')\n",
    "df_ner.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55cd15fe-faaa-4d67-be04-66e48da9eaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 19:56:37.508398: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-26 19:56:41.414392: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-26 19:56:41.414598: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-26 19:56:41.414613: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-05-26 19:56:50.823564: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-26 19:56:50.824820: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-05-26 19:56:50.824867: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tensorflow-nlp): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import _utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9badc925-ca99-4832-9b15-22a05a987284",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df_ner['clean_text_sent'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa40c3-8981-453a-8ac9-6ff86ee44e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "\n",
    "topic_model = BERTopic(language=\"english\", min_topic_size=100, n_gram_range=(1,2), calculate_probabilities=True, verbose=True)\n",
    "# Use tqdm to track progress\n",
    "with tqdm(total=len(documents), desc=\"Fitting BERTopic\") as pbar:\n",
    "    topics = topic_model.fit_transform(documents)\n",
    "    pbar.update(len(documents))\n",
    "\n",
    "# Access the probabilities separately\n",
    "#probs = topic_model.transform(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a9398e1-6fe1-4a6b-84c4-713b75ea3197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "#topic_model.save(\"bert_ner.pkl\")\n",
    "\n",
    "# Load the model from the saved file\n",
    "bert_ner = BERTopic.load(\"bert_ner.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d45c4f0-7bf9-4447-9feb-ce4fbfd337d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found: 332\n"
     ]
    }
   ],
   "source": [
    "freq = bert_ner.get_topic_info()\n",
    "\n",
    "print(f\"Topics found: {freq.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6d1e02d-72e7-41d8-8efc-10144bfadbf0",
   "metadata": {},
   "source": [
    "### Reducing the number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a9dd6-8e5e-4a34-8d7a-43d807406c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mod_BERT_pos_newr=mod_BERT_pos_newl.reduce_topics(df_filt_pos.cleaned_tokens_string.tolist(), nr_topics=60)\n",
    "mod_BERT_pos_newr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d783bce0-70dd-4877-9377-2fdd587a0ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_BERT_pos_newr.get_topic_info().head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad5ce20-1523-42f6-a86f-fea01c5180a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "bert_ner.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00565206-3cfa-4c2e-b2ef-3f083b796f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>87020</td>\n",
       "      <td>-1_ai_technology_news_new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3462</td>\n",
       "      <td>0_market_analysis_global_report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2647</td>\n",
       "      <td>1_products_resources_services_overview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2623</td>\n",
       "      <td>2_gray_gray media_media group_prnewswire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2466</td>\n",
       "      <td>3_us_newswires_presswire_us new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>2074</td>\n",
       "      <td>4_days_hours_day_healthdiseases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>1649</td>\n",
       "      <td>5_china_chinese_baidu_alibaba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>1516</td>\n",
       "      <td>6_microsoft_bing_openai_windows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>1406</td>\n",
       "      <td>7_ago_hour ago_stories_top stories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>1372</td>\n",
       "      <td>8_market_artificial intelligence_artificial_in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                               Name\n",
       "0     -1  87020                          -1_ai_technology_news_new\n",
       "1      0   3462                    0_market_analysis_global_report\n",
       "2      1   2647             1_products_resources_services_overview\n",
       "3      2   2623           2_gray_gray media_media group_prnewswire\n",
       "4      3   2466                    3_us_newswires_presswire_us new\n",
       "5      4   2074                    4_days_hours_day_healthdiseases\n",
       "6      5   1649                      5_china_chinese_baidu_alibaba\n",
       "7      6   1516                    6_microsoft_bing_openai_windows\n",
       "8      7   1406                 7_ago_hour ago_stories_top stories\n",
       "9      8   1372  8_market_artificial intelligence_artificial_in..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6141ad9-713d-4458-86d8-c347311e58d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>87020</td>\n",
       "      <td>-1_ai_technology_news_new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3462</td>\n",
       "      <td>0_market_analysis_global_report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2647</td>\n",
       "      <td>1_products_resources_services_overview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2623</td>\n",
       "      <td>2_gray_gray media_media group_prnewswire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2466</td>\n",
       "      <td>3_us_newswires_presswire_us new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>2074</td>\n",
       "      <td>4_days_hours_day_healthdiseases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>1649</td>\n",
       "      <td>5_china_chinese_baidu_alibaba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>1516</td>\n",
       "      <td>6_microsoft_bing_openai_windows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>1406</td>\n",
       "      <td>7_ago_hour ago_stories_top stories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>1372</td>\n",
       "      <td>8_market_artificial intelligence_artificial_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>1317</td>\n",
       "      <td>9_wfmztv_lehigh_berks_lehigh valley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>1277</td>\n",
       "      <td>10_npr_radio_schedule_programs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>1193</td>\n",
       "      <td>11_ct_chatgpt_school_openai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>1133</td>\n",
       "      <td>12_starfilled_starfilled starfilled_course_dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>1104</td>\n",
       "      <td>13_paid program_paid_brandvoice_brandvoice paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>1055</td>\n",
       "      <td>14_covid19_coronavirus_virus_patient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>1026</td>\n",
       "      <td>15_human_brain_ai_could</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>993</td>\n",
       "      <td>16_product_product hunt_hunt_best product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>977</td>\n",
       "      <td>17_musk_elon_elon musk_openai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>963</td>\n",
       "      <td>18_ai_data_zdnet_business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name\n",
       "0      -1  87020                          -1_ai_technology_news_new\n",
       "1       0   3462                    0_market_analysis_global_report\n",
       "2       1   2647             1_products_resources_services_overview\n",
       "3       2   2623           2_gray_gray media_media group_prnewswire\n",
       "4       3   2466                    3_us_newswires_presswire_us new\n",
       "5       4   2074                    4_days_hours_day_healthdiseases\n",
       "6       5   1649                      5_china_chinese_baidu_alibaba\n",
       "7       6   1516                    6_microsoft_bing_openai_windows\n",
       "8       7   1406                 7_ago_hour ago_stories_top stories\n",
       "9       8   1372  8_market_artificial intelligence_artificial_in...\n",
       "10      9   1317                9_wfmztv_lehigh_berks_lehigh valley\n",
       "11     10   1277                     10_npr_radio_schedule_programs\n",
       "12     11   1193                        11_ct_chatgpt_school_openai\n",
       "13     12   1133  12_starfilled_starfilled starfilled_course_dat...\n",
       "14     13   1104    13_paid program_paid_brandvoice_brandvoice paid\n",
       "15     14   1055               14_covid19_coronavirus_virus_patient\n",
       "16     15   1026                            15_human_brain_ai_could\n",
       "17     16    993          16_product_product hunt_hunt_best product\n",
       "18     17    977                      17_musk_elon_elon musk_openai\n",
       "19     18    963                          18_ai_data_zdnet_business"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_ner.get_topic_info().head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e197fd94-abc1-4f42-99c1-0095e778a5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198564"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_ner.topics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "792b964f-ae80-453f-a896-6555f0da243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner['bert_topic'] = bert_ner.topics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf03362b-6d69-4982-8022-c7caf13ec091",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = df_ner[df_ner['sentiment_label'] == 'positive']\n",
    "df_neg = df_ner[df_ner['sentiment_label'] == 'negative']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a087f50-09b0-40cf-bf10-28a9c67dbc46",
   "metadata": {},
   "source": [
    "### Taking a look at positive sentiment NERs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "656904e8-6fa4-477d-8c74-2c6270de2313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organization: AI, Count: 30985\n",
      "Organization: Gray Media Group, Count: 14170\n",
      "Organization: ChatGPT, Count: 10940\n",
      "Organization: Google, Count: 8069\n",
      "Organization: Microsoft, Count: 7950\n",
      "Organization: IBM, Count: 5825\n",
      "Organization: NVIDIA, Count: 3537\n",
      "Organization: ML, Count: 3291\n",
      "Organization: Artificial Intelligence, Count: 3146\n",
      "Organization: Gray Media Group Inc Station Gray Television Inc, Count: 2923\n",
      "Organization: BureauInvestigate, Count: 2674\n",
      "Organization: CaptioningAudio DescriptionA Gray Media Group Inc Station Gray Television Inc, Count: 2516\n",
      "Organization: Deloitte, Count: 2450\n",
      "Organization: Nvidia, Count: 2148\n",
      "Organization: Intel, Count: 2091\n",
      "Organization: Amazon, Count: 2053\n",
      "Organization: datadriven, Count: 2034\n",
      "Organization: OpenAI, Count: 1906\n",
      "Organization: DDN, Count: 1877\n",
      "Organization: Baidu, Count: 1802\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "df_ent_orgs = list(chain.from_iterable(df_pos['ENT_ORG']))\n",
    "\n",
    "counter = Counter(df_ent_orgs)\n",
    "top_20 = counter.most_common(20)\n",
    "for item, count in top_20:\n",
    "    print(f\"Organization: {item}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5b6aeef-fd6a-4a25-838e-ec381a25c011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products: AI, Count: 72337\n",
      "Products: AIdriven, Count: 1690\n",
      "Products: Reddit VKontakte Share, Count: 1306\n",
      "Products: HPC, Count: 896\n",
      "Products: Google Cloud, Count: 844\n",
      "Products: Bing, Count: 582\n",
      "Products: JavaScript, Count: 454\n",
      "Products: AI ML, Count: 420\n",
      "Products: Windows, Count: 409\n",
      "Products: Cloud, Count: 401\n",
      "Products: CRM, Count: 389\n",
      "Products: Chrome Edge, Count: 358\n",
      "Products: AI IoT, Count: 346\n",
      "Products: AgrarNetzwerktechnikl, Count: 340\n",
      "Products: Microsoft, Count: 326\n",
      "Products: Galaxy, Count: 314\n",
      "Products: Highspot, Count: 314\n",
      "Products: Key, Count: 299\n",
      "Products: NewscastsPress, Count: 288\n",
      "Products: DGX, Count: 288\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "df_ent_prods = list(chain.from_iterable(df_pos['ENT_PROD']))\n",
    "\n",
    "counter = Counter(df_ent_prods)\n",
    "top_20 = counter.most_common(20)\n",
    "for item, count in top_20:\n",
    "    print(f\"Products: {item}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eef1d70c-bbe8-429a-893f-1f528c6af68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person: Greta Van SusterenCircle Country, Count: 1099\n",
      "Person: Instagram, Count: 736\n",
      "Person: Elon Musk, Count: 689\n",
      "Person: CaptioningAudio, Count: 681\n",
      "Person: BureauInvestigate, Count: 675\n",
      "Person: Bing, Count: 580\n",
      "Person: Porter, Count: 558\n",
      "Person: Musk, Count: 500\n",
      "Person: Firmenich, Count: 487\n",
      "Person: Hailo, Count: 429\n",
      "Person: Biden, Count: 426\n",
      "Person: Trump, Count: 413\n",
      "Person: Laivly, Count: 387\n",
      "Person: BlickpunktAd hocMitteilungenBestbewertete NewsMeistgelesene, Count: 383\n",
      "Person: JAIC, Count: 382\n",
      "Person: Standigm, Count: 378\n",
      "Person: WITN, Count: 357\n",
      "Person: Frost Sullivan, Count: 355\n",
      "Person: Seite, Count: 354\n",
      "Person: sehr gut123456schlechtProblem, Count: 353\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "df_ent_pers = list(chain.from_iterable(df_pos['ENT_PER']))\n",
    "\n",
    "counter = Counter(df_ent_pers)\n",
    "top_20 = counter.most_common(20)\n",
    "for item, count in top_20:\n",
    "    print(f\"Person: {item}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e032fd6-430c-43a2-b3a7-06de0c537bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organization: AI, Count: 5667\n",
      "Organization: ChatGPT, Count: 5251\n",
      "Organization: Google, Count: 4134\n",
      "Organization: Machine Learning, Count: 2067\n",
      "Organization: Microsoft, Count: 1856\n",
      "Organization: Lisk Machine Learning, Count: 1742\n",
      "Organization: Matrix AI Network, Count: 1393\n",
      "Organization: OpenAI, Count: 935\n",
      "Organization: Bard, Count: 929\n",
      "Organization: MarketBeatcom, Count: 829\n",
      "Organization: Twitter, Count: 724\n",
      "Organization: Link Machine Learning, Count: 720\n",
      "Organization: Ethereum, Count: 656\n",
      "Organization: Apple, Count: 643\n",
      "Organization: ABMN, Count: 559\n",
      "Organization: CryptoBeat, Count: 548\n",
      "Organization: EU, Count: 537\n",
      "Organization: Baidu, Count: 469\n",
      "Organization: CNN, Count: 462\n",
      "Organization: Tesla, Count: 421\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "df_ent_orgs = list(chain.from_iterable(df_neg['ENT_ORG']))\n",
    "\n",
    "counter = Counter(df_ent_orgs)\n",
    "top_20 = counter.most_common(20)\n",
    "for item, count in top_20:\n",
    "    print(f\"Organization: {item}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b505b1b-8c33-4578-9f43-89095c0ddbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products: AI, Count: 9767\n",
      "Products: Bing, Count: 262\n",
      "Products: AI Enhanced, Count: 129\n",
      "Products: JavaScript, Count: 107\n",
      "Products: Atrioc, Count: 101\n",
      "Products: AIdriven, Count: 96\n",
      "Products: ChatGPTstyle, Count: 92\n",
      "Products: Windows, Count: 85\n",
      "Products: YouTube, Count: 79\n",
      "Products: DelaysElection ResultsLive, Count: 76\n",
      "Products: Air Stations Air, Count: 71\n",
      "Products: Galaxy, Count: 69\n",
      "Products: C3ai, Count: 58\n",
      "Products: SpaceX, Count: 52\n",
      "Products: AI Complete, Count: 52\n",
      "Products: Matrix AI Network, Count: 51\n",
      "Products: Open AI, Count: 50\n",
      "Products: Blueprint Prep, Count: 45\n",
      "Products: DeepMind, Count: 45\n",
      "Products: Opens, Count: 44\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "df_ent_prods = list(chain.from_iterable(df_neg['ENT_PROD']))\n",
    "\n",
    "counter = Counter(df_ent_prods)\n",
    "top_20 = counter.most_common(20)\n",
    "for item, count in top_20:\n",
    "    print(f\"Products: {item}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ab65853-af2b-41e2-81b4-8952357eaa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person: Vectorspace AI, Count: 802\n",
      "Person: Elon Musk, Count: 562\n",
      "Person: Musk, Count: 510\n",
      "Person: Noelle Martin, Count: 342\n",
      "Person: Martin, Count: 308\n",
      "Person: Jones, Count: 306\n",
      "Person: Biden, Count: 249\n",
      "Person: Bing, Count: 229\n",
      "Person: Trump, Count: 201\n",
      "Person: Sundar Pichai, Count: 178\n",
      "Person: Sam Altman, Count: 174\n",
      "Person: Ernie Bot, Count: 152\n",
      "Person: Bard, Count: 142\n",
      "Person: Joe Biden, Count: 139\n",
      "Person: Snapchat, Count: 124\n",
      "Person: Dai DAI, Count: 124\n",
      "Person: Donald Trump, Count: 123\n",
      "Person: AI, Count: 121\n",
      "Person: Midjourney, Count: 115\n",
      "Person: Vectorspace, Count: 113\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "df_ent_pers = list(chain.from_iterable(df_neg['ENT_PER']))\n",
    "\n",
    "counter = Counter(df_ent_pers)\n",
    "top_20 = counter.most_common(20)\n",
    "for item, count in top_20:\n",
    "    print(f\"Person: {item}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6aaf8de2-02b5-4c68-8b7b-93e554f6c5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30254, 18)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "732a83a2-122c-4763-8539-267cbc14c180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4410, 18)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_neg.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "890814cc-3e7f-4983-bc2d-d192d8b8641b",
   "metadata": {},
   "source": [
    "### redoing sentiment using roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47898390-3e3e-41af-926d-0c3f5cf12dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = 'roberta-base'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a379d04-e13e-44c9-9392-a44f6013aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    tokens = tokenizer.encode_plus(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)[0]\n",
    "        predicted_class = torch.argmax(probabilities).item()\n",
    "\n",
    "    negative_threshold = 0.3  # Example threshold value for negative sentiment\n",
    "    if probabilities[0] >= negative_threshold:\n",
    "        sentiment = \"Positive\"\n",
    "    elif probabilities[1] >= negative_threshold:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "\n",
    "    return sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ce5110e-e1e1-4ad3-a20a-12aa9842edb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>year_month</th>\n",
       "      <th>clean_text_case</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text_sent</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>formatted_date</th>\n",
       "      <th>clean_text_case_sent</th>\n",
       "      <th>entities_spacy</th>\n",
       "      <th>ENT_ORG</th>\n",
       "      <th>ENT_PROD</th>\n",
       "      <th>ENT_PER</th>\n",
       "      <th>ENT_NORP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-18</td>\n",
       "      <td>Artificial intelligence improves parking effic...</td>\n",
       "      <td>\\n\\nArtificial intelligence improves parking e...</td>\n",
       "      <td>Mar 2021</td>\n",
       "      <td>[Artificial, intelligence, improves, parking, ...</td>\n",
       "      <td>[artificial, intelligence, improves, parking, ...</td>\n",
       "      <td>artificial intelligence improves parking effic...</td>\n",
       "      <td>artificial intelligence improves parking effic...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.997408</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>Artificial intelligence improves parking effic...</td>\n",
       "      <td>[[Chinese, NORP], [Chinese, NORP], [Japanese, ...</td>\n",
       "      <td>[ETC, ETC, ETC, ETC, ETC, Wang, ETC, AIpark Sk...</td>\n",
       "      <td>[AI, AI, AI, AI, AI]</td>\n",
       "      <td>[Wang life, AIpark Sky Eye, Xiang Yanping, AIp...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>Children With Autism Saw Their Learning and So...</td>\n",
       "      <td>\\nChildren With Autism Saw Their Learning and ...</td>\n",
       "      <td>Feb 2020</td>\n",
       "      <td>[Children, Autism, Saw, Learning, Social, Skil...</td>\n",
       "      <td>[children, autism, saw, learning, social, skil...</td>\n",
       "      <td>child autism saw learning social skill boosted...</td>\n",
       "      <td>children autism saw learning social skills boo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.999264</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>Children Autism Saw Learning Social Skills Boo...</td>\n",
       "      <td>[[Thursday, DATE], [Drax, ORG], [March, DATE],...</td>\n",
       "      <td>[Drax, Italy Coronavirus Dettol, News Parliame...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Levi Strauss, Mansplaining, Levi Strauss, Sho...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-26</td>\n",
       "      <td>Forget ML, AI and Industry 4.0 – obsolescence ...</td>\n",
       "      <td>\\n\\nForget ML, AI and Industry 4.0 – obsolesce...</td>\n",
       "      <td>Mar 2021</td>\n",
       "      <td>[Forget, ML, AI, Industry, 40, obsolescence, f...</td>\n",
       "      <td>[forget, ml, ai, industry, 40, obsolescence, f...</td>\n",
       "      <td>forget ml ai industry 40 – obsolescence focus ...</td>\n",
       "      <td>forget ml ai industry 40 obsolescence focus fe...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.999864</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>Forget ML AI Industry 40 obsolescence focus Fe...</td>\n",
       "      <td>[[Forget ML AI Industry, ORG], [40, CARDINAL],...</td>\n",
       "      <td>[Forget ML AI Industry, Signal LSI Circuit Sys...</td>\n",
       "      <td>[Britishmade, Throughhole, lowtemp]</td>\n",
       "      <td>[Willian Santos, Durafuse LT]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-10</td>\n",
       "      <td>Strategy Analytics: 71% of Smartphones Sold Gl...</td>\n",
       "      <td>\\n\\nStrategy Analytics: 71% of Smartphones Sol...</td>\n",
       "      <td>Mar 2021</td>\n",
       "      <td>[Strategy, Analytics, Smartphones, Sold, Globa...</td>\n",
       "      <td>[strategy, analytics, smartphones, sold, globa...</td>\n",
       "      <td>strategy analytics 71 smartphones sold globall...</td>\n",
       "      <td>strategy analytics smartphones sold globally a...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.999874</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>Strategy Analytics Smartphones Sold Globally A...</td>\n",
       "      <td>[[AI Powered hour ago, TIME], [Artificial Inte...</td>\n",
       "      <td>[Artificial Intelligence Powers, Smartphones B...</td>\n",
       "      <td>[AI, ondevice AI, AI, AI, AI, AI, AI, AI, Pro ...</td>\n",
       "      <td>[Ken Hyers, Ukonaho, Ken Hyers, Ukonaho Camera...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-10-20</td>\n",
       "      <td>Olympus to Support Endoscopic AI Diagnosis Edu...</td>\n",
       "      <td>\\n\\nOlympus to Support Endoscopic AI Diagnosis...</td>\n",
       "      <td>Oct 2020</td>\n",
       "      <td>[Olympus, Support, Endoscopic, AI, Diagnosis, ...</td>\n",
       "      <td>[olympus, support, endoscopic, ai, diagnosis, ...</td>\n",
       "      <td>olympus support endoscopic ai diagnosis educat...</td>\n",
       "      <td>olympus support endoscopic ai diagnosis educat...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.999858</td>\n",
       "      <td>2020-10-01</td>\n",
       "      <td>Olympus Support Endoscopic AI Diagnosis Educat...</td>\n",
       "      <td>[[Olympus Support Endoscopic AI Diagnosis Educ...</td>\n",
       "      <td>[Olympus Support Endoscopic AI Diagnosis Educa...</td>\n",
       "      <td>[AI, AI, AI]</td>\n",
       "      <td>[Satoshi Hemmi, Yuka Horimoto819024901071]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                              title  \\\n",
       "0 2021-03-18  Artificial intelligence improves parking effic...   \n",
       "1 2020-02-27  Children With Autism Saw Their Learning and So...   \n",
       "2 2021-03-26  Forget ML, AI and Industry 4.0 – obsolescence ...   \n",
       "3 2021-03-10  Strategy Analytics: 71% of Smartphones Sold Gl...   \n",
       "4 2020-10-20  Olympus to Support Endoscopic AI Diagnosis Edu...   \n",
       "\n",
       "                                                text year_month  \\\n",
       "0  \\n\\nArtificial intelligence improves parking e...   Mar 2021   \n",
       "1  \\nChildren With Autism Saw Their Learning and ...   Feb 2020   \n",
       "2  \\n\\nForget ML, AI and Industry 4.0 – obsolesce...   Mar 2021   \n",
       "3  \\n\\nStrategy Analytics: 71% of Smartphones Sol...   Mar 2021   \n",
       "4  \\n\\nOlympus to Support Endoscopic AI Diagnosis...   Oct 2020   \n",
       "\n",
       "                                     clean_text_case  \\\n",
       "0  [Artificial, intelligence, improves, parking, ...   \n",
       "1  [Children, Autism, Saw, Learning, Social, Skil...   \n",
       "2  [Forget, ML, AI, Industry, 40, obsolescence, f...   \n",
       "3  [Strategy, Analytics, Smartphones, Sold, Globa...   \n",
       "4  [Olympus, Support, Endoscopic, AI, Diagnosis, ...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  [artificial, intelligence, improves, parking, ...   \n",
       "1  [children, autism, saw, learning, social, skil...   \n",
       "2  [forget, ml, ai, industry, 40, obsolescence, f...   \n",
       "3  [strategy, analytics, smartphones, sold, globa...   \n",
       "4  [olympus, support, endoscopic, ai, diagnosis, ...   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0  artificial intelligence improves parking effic...   \n",
       "1  child autism saw learning social skill boosted...   \n",
       "2  forget ml ai industry 40 – obsolescence focus ...   \n",
       "3  strategy analytics 71 smartphones sold globall...   \n",
       "4  olympus support endoscopic ai diagnosis educat...   \n",
       "\n",
       "                                     clean_text_sent sentiment_label  \\\n",
       "0  artificial intelligence improves parking effic...        positive   \n",
       "1  children autism saw learning social skills boo...        positive   \n",
       "2  forget ml ai industry 40 obsolescence focus fe...         neutral   \n",
       "3  strategy analytics smartphones sold globally a...         neutral   \n",
       "4  olympus support endoscopic ai diagnosis educat...         neutral   \n",
       "\n",
       "   sentiment_score formatted_date  \\\n",
       "0         0.997408     2021-03-01   \n",
       "1         0.999264     2020-02-01   \n",
       "2         0.999864     2021-03-01   \n",
       "3         0.999874     2021-03-01   \n",
       "4         0.999858     2020-10-01   \n",
       "\n",
       "                                clean_text_case_sent  \\\n",
       "0  Artificial intelligence improves parking effic...   \n",
       "1  Children Autism Saw Learning Social Skills Boo...   \n",
       "2  Forget ML AI Industry 40 obsolescence focus Fe...   \n",
       "3  Strategy Analytics Smartphones Sold Globally A...   \n",
       "4  Olympus Support Endoscopic AI Diagnosis Educat...   \n",
       "\n",
       "                                      entities_spacy  \\\n",
       "0  [[Chinese, NORP], [Chinese, NORP], [Japanese, ...   \n",
       "1  [[Thursday, DATE], [Drax, ORG], [March, DATE],...   \n",
       "2  [[Forget ML AI Industry, ORG], [40, CARDINAL],...   \n",
       "3  [[AI Powered hour ago, TIME], [Artificial Inte...   \n",
       "4  [[Olympus Support Endoscopic AI Diagnosis Educ...   \n",
       "\n",
       "                                             ENT_ORG  \\\n",
       "0  [ETC, ETC, ETC, ETC, ETC, Wang, ETC, AIpark Sk...   \n",
       "1  [Drax, Italy Coronavirus Dettol, News Parliame...   \n",
       "2  [Forget ML AI Industry, Signal LSI Circuit Sys...   \n",
       "3  [Artificial Intelligence Powers, Smartphones B...   \n",
       "4  [Olympus Support Endoscopic AI Diagnosis Educa...   \n",
       "\n",
       "                                            ENT_PROD  \\\n",
       "0                               [AI, AI, AI, AI, AI]   \n",
       "1                                                 []   \n",
       "2                [Britishmade, Throughhole, lowtemp]   \n",
       "3  [AI, ondevice AI, AI, AI, AI, AI, AI, AI, Pro ...   \n",
       "4                                       [AI, AI, AI]   \n",
       "\n",
       "                                             ENT_PER ENT_NORP  \n",
       "0  [Wang life, AIpark Sky Eye, Xiang Yanping, AIp...       []  \n",
       "1  [Levi Strauss, Mansplaining, Levi Strauss, Sho...       []  \n",
       "2                      [Willian Santos, Durafuse LT]       []  \n",
       "3  [Ken Hyers, Ukonaho, Ken Hyers, Ukonaho Camera...       []  \n",
       "4         [Satoshi Hemmi, Yuka Horimoto819024901071]       []  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ner.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6b383f-28a3-411c-8ecb-063e7224cfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tqdm.pandas( )\n",
    "#df_ner[\"sentiment_roberta\"] = df_ner['clean_text_sent'].parallel_apply(lambda x: analyze_sentiment(x)).progress_apply(lambda x: x)\n",
    "\n",
    "df_ner['sentiment_roberta'] = df_ner['clean_text_sent'].progress_apply(analyze_sentiment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb06904-d958-419d-bab7-76eba085332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner.to_parquet('roberta_sentiment_ner.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ff05d-08ba-46e7-b64b-af4c165604c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_ner = pd.read_parquet('roberta_sentiment_ner.parquet', engine='pyarrow')\n",
    "df_ner.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02d561ed-99d8-4565-a583-fd83ce852cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the pre-trained sentiment analysis model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "thresholds = {\n",
    "'LABEL_0': 0.05, # Negative threshold\n",
    "'LABEL_1': 0.9, # Neutral threshold\n",
    "'LABEL_2': 0.05 } # Positive threshold\n",
    "    \n",
    "# Function to perform sentiment analysis on a text with custom thresholds\n",
    "def analyze_sentiment(text):\n",
    "    \n",
    "    encoded_input = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_input)\n",
    "    #output = model(**encoded_input)\n",
    "    #scores = output.logits.detach().numpy()\n",
    "    #probabilities = torch.softmax(scores, axis=-1)[0]\n",
    "    probabilities = torch.softmax(outputs.logits, dim=1)[0]\n",
    "    \n",
    "    label_probs = {f'LABEL_{i}': prob for i, prob in enumerate(probabilities)}\n",
    "    \n",
    "    # Assign the label with the highest probability that exceeds its threshold\n",
    "    sentiment_label = 'unclassified' # Default label if no thresholds are exceeded\n",
    "    for label, prob in label_probs.items ():\n",
    "        if prob > thresholds[label]: # Check if the probability exceeds the threshold\n",
    "            sentiment_label = label\n",
    "            break # Stop checking other labels\n",
    "    return sentiment_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8723c5c4-2d3e-4d1d-b73c-e10292173eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# reference\n",
    "\n",
    "# Assuming you have a very large database stored in the 'df_ner' DataFrame\n",
    "chunk_size = 5000\n",
    "\n",
    "# Iterate over chunks of the DataFrame\n",
    "for chunk_start in tqdm(range(0, len(df_ner), chunk_size)):\n",
    "    chunk_end = min(chunk_start + chunk_size, len(df_ner))\n",
    "    chunk = df_ner.iloc[chunk_start:chunk_end]\n",
    "\n",
    "    # Apply sentiment analysis to the 'text' column using the analyze_sentiment function\n",
    "    chunk['sentiment_label_roberta'] = chunk['clean_title'].parallel_apply(analyze_sentiment).apply(pd.Series)\n",
    "\n",
    "    # Update the original DataFrame with the sentiment analysis results\n",
    "    df_ner.iloc[chunk_start:chunk_end] = chunk\n",
    "\n",
    "# Print the updated DataFrame with sentiment analysis results\n",
    "print(df_ner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "558fba7f-e78f-4f1c-8b3f-ffdb13312d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>year_month</th>\n",
       "      <th>clean_text_case</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_text_sent</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>formatted_date</th>\n",
       "      <th>clean_text_case_sent</th>\n",
       "      <th>entities_spacy</th>\n",
       "      <th>ENT_ORG</th>\n",
       "      <th>ENT_PROD</th>\n",
       "      <th>ENT_PER</th>\n",
       "      <th>ENT_NORP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-18</td>\n",
       "      <td>Artificial intelligence improves parking effic...</td>\n",
       "      <td>\\n\\nArtificial intelligence improves parking e...</td>\n",
       "      <td>Mar 2021</td>\n",
       "      <td>[Artificial, intelligence, improves, parking, ...</td>\n",
       "      <td>[artificial, intelligence, improves, parking, ...</td>\n",
       "      <td>artificial intelligence improves parking effic...</td>\n",
       "      <td>artificial intelligence improves parking effic...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.997408</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>Artificial intelligence improves parking effic...</td>\n",
       "      <td>[[Chinese, NORP], [Chinese, NORP], [Japanese, ...</td>\n",
       "      <td>[ETC, ETC, ETC, ETC, ETC, Wang, ETC, AIpark Sk...</td>\n",
       "      <td>[AI, AI, AI, AI, AI]</td>\n",
       "      <td>[Wang life, AIpark Sky Eye, Xiang Yanping, AIp...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>Children With Autism Saw Their Learning and So...</td>\n",
       "      <td>\\nChildren With Autism Saw Their Learning and ...</td>\n",
       "      <td>Feb 2020</td>\n",
       "      <td>[Children, Autism, Saw, Learning, Social, Skil...</td>\n",
       "      <td>[children, autism, saw, learning, social, skil...</td>\n",
       "      <td>child autism saw learning social skill boosted...</td>\n",
       "      <td>children autism saw learning social skills boo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.999264</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>Children Autism Saw Learning Social Skills Boo...</td>\n",
       "      <td>[[Thursday, DATE], [Drax, ORG], [March, DATE],...</td>\n",
       "      <td>[Drax, Italy Coronavirus Dettol, News Parliame...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Levi Strauss, Mansplaining, Levi Strauss, Sho...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-26</td>\n",
       "      <td>Forget ML, AI and Industry 4.0 – obsolescence ...</td>\n",
       "      <td>\\n\\nForget ML, AI and Industry 4.0 – obsolesce...</td>\n",
       "      <td>Mar 2021</td>\n",
       "      <td>[Forget, ML, AI, Industry, 40, obsolescence, f...</td>\n",
       "      <td>[forget, ml, ai, industry, 40, obsolescence, f...</td>\n",
       "      <td>forget ml ai industry 40 – obsolescence focus ...</td>\n",
       "      <td>forget ml ai industry 40 obsolescence focus fe...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.999864</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>Forget ML AI Industry 40 obsolescence focus Fe...</td>\n",
       "      <td>[[Forget ML AI Industry, ORG], [40, CARDINAL],...</td>\n",
       "      <td>[Forget ML AI Industry, Signal LSI Circuit Sys...</td>\n",
       "      <td>[Britishmade, Throughhole, lowtemp]</td>\n",
       "      <td>[Willian Santos, Durafuse LT]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                              title  \\\n",
       "0 2021-03-18  Artificial intelligence improves parking effic...   \n",
       "1 2020-02-27  Children With Autism Saw Their Learning and So...   \n",
       "2 2021-03-26  Forget ML, AI and Industry 4.0 – obsolescence ...   \n",
       "\n",
       "                                                text year_month  \\\n",
       "0  \\n\\nArtificial intelligence improves parking e...   Mar 2021   \n",
       "1  \\nChildren With Autism Saw Their Learning and ...   Feb 2020   \n",
       "2  \\n\\nForget ML, AI and Industry 4.0 – obsolesce...   Mar 2021   \n",
       "\n",
       "                                     clean_text_case  \\\n",
       "0  [Artificial, intelligence, improves, parking, ...   \n",
       "1  [Children, Autism, Saw, Learning, Social, Skil...   \n",
       "2  [Forget, ML, AI, Industry, 40, obsolescence, f...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  [artificial, intelligence, improves, parking, ...   \n",
       "1  [children, autism, saw, learning, social, skil...   \n",
       "2  [forget, ml, ai, industry, 40, obsolescence, f...   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0  artificial intelligence improves parking effic...   \n",
       "1  child autism saw learning social skill boosted...   \n",
       "2  forget ml ai industry 40 – obsolescence focus ...   \n",
       "\n",
       "                                     clean_text_sent sentiment_label  \\\n",
       "0  artificial intelligence improves parking effic...        positive   \n",
       "1  children autism saw learning social skills boo...        positive   \n",
       "2  forget ml ai industry 40 obsolescence focus fe...         neutral   \n",
       "\n",
       "   sentiment_score formatted_date  \\\n",
       "0         0.997408     2021-03-01   \n",
       "1         0.999264     2020-02-01   \n",
       "2         0.999864     2021-03-01   \n",
       "\n",
       "                                clean_text_case_sent  \\\n",
       "0  Artificial intelligence improves parking effic...   \n",
       "1  Children Autism Saw Learning Social Skills Boo...   \n",
       "2  Forget ML AI Industry 40 obsolescence focus Fe...   \n",
       "\n",
       "                                      entities_spacy  \\\n",
       "0  [[Chinese, NORP], [Chinese, NORP], [Japanese, ...   \n",
       "1  [[Thursday, DATE], [Drax, ORG], [March, DATE],...   \n",
       "2  [[Forget ML AI Industry, ORG], [40, CARDINAL],...   \n",
       "\n",
       "                                             ENT_ORG  \\\n",
       "0  [ETC, ETC, ETC, ETC, ETC, Wang, ETC, AIpark Sk...   \n",
       "1  [Drax, Italy Coronavirus Dettol, News Parliame...   \n",
       "2  [Forget ML AI Industry, Signal LSI Circuit Sys...   \n",
       "\n",
       "                              ENT_PROD  \\\n",
       "0                 [AI, AI, AI, AI, AI]   \n",
       "1                                   []   \n",
       "2  [Britishmade, Throughhole, lowtemp]   \n",
       "\n",
       "                                             ENT_PER ENT_NORP  \n",
       "0  [Wang life, AIpark Sky Eye, Xiang Yanping, AIp...       []  \n",
       "1  [Levi Strauss, Mansplaining, Levi Strauss, Sho...       []  \n",
       "2                      [Willian Santos, Durafuse LT]       []  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ner.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83145aa0-a12c-4478-9729-fba0986b35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner['ENT_NORP'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76fde251-4e68-43de-9ce9-48c5a055c927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows:  198564\n",
      "Drop text dups:  197812\n",
      "Drop title, sentiment dups:  141130\n"
     ]
    }
   ],
   "source": [
    "print('Total number of rows: ',df_ner.shape[0])\n",
    "\n",
    "## Dropping duplicate news articles\n",
    "df1 = df_ner.drop_duplicates(subset=['clean_text_sent'], keep='first', inplace=False)\n",
    "print('Drop text dups: ',df1.shape[0])\n",
    "\n",
    "## Dropping news titles that report articles with the same sentiment\n",
    "df2 = df1.drop_duplicates(subset=['title','sentiment_label'], keep='first', inplace=False)\n",
    "print('Drop title, sentiment dups: ',df2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af440232-1aa3-4b15-be8b-33b4103577fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141130/141130 [12:09:26<00:00,  3.22it/s]  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tqdm.pandas( )\n",
    "df2['sentiment_label_roberta'] = df2['clean_title'].progress_apply(analyze_sentiment).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c93d47ba-9293-494e-a663-e6ef96fc668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_parquet('06_sent_roberta_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "406d96ad-82e2-41bd-8bf7-b144be13d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_parquet('06_sent_roberta_df.parquet', engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e14e9fcb-42cb-4f31-9201-dc7ae37961bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[74438, 53250, 13442]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['sentiment_label_roberta'].value_counts().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b743f59-3b99-4457-8de7-e554f1f63197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LABEL_2    74438\n",
       "LABEL_0    53250\n",
       "LABEL_1    13442\n",
       "Name: sentiment_label_roberta, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['sentiment_label_roberta'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2723f86b-662f-4fb9-8508-60a6fd8c3763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"156.49625pt\" height=\"200.865938pt\" viewBox=\"0 0 156.49625 200.865938\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-05-26T13:54:08.573839</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.6.3, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 200.865938 \n",
       "L 156.49625 200.865938 \n",
       "L 156.49625 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 37.69625 173.52 \n",
       "L 149.29625 173.52 \n",
       "L 149.29625 7.2 \n",
       "L 37.69625 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"text_1\">\n",
       "      <!-- Sentiment -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(65.067266 191.378281) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-53\" d=\"M 3425 4513 \n",
       "L 3425 3897 \n",
       "Q 3066 4069 2747 4153 \n",
       "Q 2428 4238 2131 4238 \n",
       "Q 1616 4238 1336 4038 \n",
       "Q 1056 3838 1056 3469 \n",
       "Q 1056 3159 1242 3001 \n",
       "Q 1428 2844 1947 2747 \n",
       "L 2328 2669 \n",
       "Q 3034 2534 3370 2195 \n",
       "Q 3706 1856 3706 1288 \n",
       "Q 3706 609 3251 259 \n",
       "Q 2797 -91 1919 -91 \n",
       "Q 1588 -91 1214 -16 \n",
       "Q 841 59 441 206 \n",
       "L 441 856 \n",
       "Q 825 641 1194 531 \n",
       "Q 1563 422 1919 422 \n",
       "Q 2459 422 2753 634 \n",
       "Q 3047 847 3047 1241 \n",
       "Q 3047 1584 2836 1778 \n",
       "Q 2625 1972 2144 2069 \n",
       "L 1759 2144 \n",
       "Q 1053 2284 737 2584 \n",
       "Q 422 2884 422 3419 \n",
       "Q 422 4038 858 4394 \n",
       "Q 1294 4750 2059 4750 \n",
       "Q 2388 4750 2728 4690 \n",
       "Q 3069 4631 3425 4513 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-65\" x=\"63.476562\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-6e\" x=\"125\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-74\" x=\"188.378906\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-69\" x=\"227.587891\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-6d\" x=\"255.371094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-65\" x=\"352.783203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-6e\" x=\"414.306641\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-74\" x=\"477.685547\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 0 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(21.1975 177.699141) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 20 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(14.19875 145.987429) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 40 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(14.19875 114.275717) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 60 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(14.19875 82.564005) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 80 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(14.19875 50.852294) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 100 -->\n",
       "      <g style=\"fill: #262626\" transform=\"translate(7.2 19.140582) scale(0.11 -0.11)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 42.768977 173.52 \n",
       "L 144.223523 173.52 \n",
       "L 144.223523 89.95964 \n",
       "L 42.768977 89.95964 \n",
       "z\n",
       "\" clip-path=\"url(#p6486f07ae9)\" style=\"fill: #554745; stroke: #ffffff; stroke-linejoin: miter\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 42.768977 89.95964 \n",
       "L 144.223523 89.95964 \n",
       "L 144.223523 30.183063 \n",
       "L 42.768977 30.183063 \n",
       "z\n",
       "\" clip-path=\"url(#p6486f07ae9)\" style=\"fill: #816c68; stroke: #ffffff; stroke-linejoin: miter\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 42.768977 30.183063 \n",
       "L 144.223523 30.183063 \n",
       "L 144.223523 15.12 \n",
       "L 42.768977 15.12 \n",
       "z\n",
       "\" clip-path=\"url(#p6486f07ae9)\" style=\"fill: #f5f2ee; stroke: #ffffff; stroke-linejoin: miter\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 37.69625 173.52 \n",
       "L 37.69625 7.2 \n",
       "\" style=\"fill: none; stroke: #262626; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 37.69625 173.52 \n",
       "L 149.29625 173.52 \n",
       "\" style=\"fill: none; stroke: #262626; stroke-width: 1.25; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p6486f07ae9\">\n",
       "   <rect x=\"37.69625\" y=\"7.2\" width=\"111.6\" height=\"166.32\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 200x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the labels and values\n",
    "labels = ['Positive', 'Negative', 'Neutral']\n",
    "sizes = df2['sentiment_label_roberta'].value_counts().to_list()\n",
    "size_pct = [52.7, 37.7, 9.5]\n",
    "# Define the custom colors\n",
    "custom_colors = [\"#554745\", \"#816c68\", \"#f5f2ee\"]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [2,3]\n",
    "\n",
    " \n",
    "# create data\n",
    "x = ['Sentiment']\n",
    "y1 = [10, 20, 10, 30]\n",
    "\n",
    "y1 = np.array([52.7])\n",
    "y2 = np.array([37.7])\n",
    "y3 = np.array([9.5])\n",
    " \n",
    "# plot bars in stack manner\n",
    "bars=[]\n",
    "bars.append(plt.bar(x, y1, color=custom_colors[0]))\n",
    "bars.append(plt.bar(x, y2, bottom=y1, color=custom_colors[1]))\n",
    "bars.append(plt.bar(x, y3, bottom=y1+y2, color=custom_colors[2]))\n",
    "\n",
    "# Remove the chart border\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5b3c660-a7ff-4529-a513-fe3815fad8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127688, 18)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_sent = df2[df2['sentiment_label_roberta'] != 'LABEL_1']\n",
    "df2_sent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aa63014-28f0-49d2-9138-30cbbae04b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_pos = df2[df2['sentiment_label_roberta'] == 'LABEL_2']\n",
    "df2_neg = df2[df2['sentiment_label_roberta'] == 'LABEL_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86f14156-e3ea-4a3d-8d4f-833c33c1cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_pos = df2_pos['clean_text_sent'].tolist()\n",
    "documents_neg = df2_neg['clean_text_sent'].tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "461c569d-525e-433f-9938-d312d10c91ea",
   "metadata": {},
   "source": [
    "### Topic Modelling on Positive text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7d7c6f-1f59-4556-9bb4-e99b9cafbc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "\n",
    "pos_topic_model = BERTopic(language=\"english\", min_topic_size=100, n_gram_range=(1,2), calculate_probabilities=True, verbose=True)\n",
    "# Use tqdm to track progress\n",
    "with tqdm(total=len(documents_pos), desc=\"Fitting BERTopic\") as pbar:\n",
    "    topics = pos_topic_model.fit_transform(documents_pos)\n",
    "    pbar.update(len(documents_pos))\n",
    "\n",
    "# Access the probabilities separately\n",
    "pos_probs = pos_topic_model.transform(documents_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7fcd9b8-bd45-4afe-ac4b-4744e8fd24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "pos_topic_model.save(\"roberta_pos.pkl\")\n",
    "\n",
    "# Load the model from the saved file\n",
    "loaded_model_pos = BERTopic.load(\"roberta_pos.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b8d1d4-0dd4-4ac8-a193-8022b41c863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = pos_topic_model.topics_, pos_topic_model.probabilities_\n",
    "df2_pos['bert_topic'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccd08501-ccca-4572-b6b2-22bb80276d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found: 139\n",
      "CPU times: user 4.57 ms, sys: 150 µs, total: 4.72 ms\n",
      "Wall time: 3.95 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "freq = loaded_model_pos.get_topic_info()\n",
    "\n",
    "print(f\"Topics found: {freq.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12259e17-2f14-4e30-964c-477106eabd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.head(20)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
