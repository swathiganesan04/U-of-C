{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gdBBRZugIQGd"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CAIzdiOFKvDo",
    "outputId": "ee15cdca-969b-46a3-90ee-2d6a37129b83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sq6EEGwYY_5H"
   },
   "source": [
    "### 1 Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sh1pMw6yKvck",
    "outputId": "40a57df1-b083-43e9-9cbe-c24ba628cfb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "    '/content/gdrive/My Drive/dataset_train',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    classes=['category 1', 'category 2', 'category 3', 'category 4'],\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PTvUO7hBY_5I",
    "outputId": "698cd8a7-42be-497c-bc5e-829e217614ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.image_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkjcmeU7Y_5I"
   },
   "source": [
    "The <b>image shape</b> of each training observation is <b>(64, 64, 3)</b>\n",
    "An image shape of 64 x 64 x 3 means that the image is a square with a height and width of 64 pixels, and has 3 color channels: red, green, and blue (RGB). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbBA2K4xY_5I",
    "outputId": "d753fee8-5063-4adf-d289-f7edc87f6386"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_set.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5205A09Y_5J"
   },
   "source": [
    "Our train set has <b>4 classes</b> to predict on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTr6WF8XY_5J"
   },
   "source": [
    "### 2 Initial Classifier Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "P6QqAGRmUWTY"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Create the classifier instance\n",
    "classifier = Sequential()\n",
    "\n",
    "# Add the first convolutional layer\n",
    "classifier.add(Conv2D(filters=32, kernel_size=(3,3), input_shape=(64, 64, 3), activation='relu'))\n",
    "\n",
    "# Add the first pooling layer\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Add the second convolutional layer\n",
    "classifier.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
    "\n",
    "# Add the second pooling layer\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Flatten the output from the convolutional layers\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Add the first fully connected layer\n",
    "classifier.add(Dense(units=128, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "classifier.add(Dense(units=4, activation='softmax'))\n",
    "\n",
    "# Compile the classifier\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgxoG46DY_5J"
   },
   "source": [
    "### 3 Model Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46t8y-hsUrpj",
    "outputId": "38971f95-0f2a-4457-cb0a-1f368b8ba454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3/3 [==============================] - 3s 477ms/step - loss: 1.4918 - accuracy: 0.2955\n",
      "Epoch 2/3\n",
      "3/3 [==============================] - 2s 590ms/step - loss: 1.0548 - accuracy: 0.5227\n",
      "Epoch 3/3\n",
      "3/3 [==============================] - 1s 318ms/step - loss: 0.5776 - accuracy: 0.8636\n",
      "Saved model\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "classifier.fit(\n",
    "    train_set,\n",
    "    steps_per_epoch=3,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "# Save the model to your Google Drive\n",
    "classifier.save('/content/gdrive/My Drive/model/img_class.h5')\n",
    "print(\"Saved model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VT8coy55VBoj",
    "outputId": "85566d6b-5fb3-45ce-8b59-00e07bf06b98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "/content/gdrive/My Drive/dataset_test/4011.png\n",
      "/content/gdrive/My Drive/dataset_test/1022.png\n",
      "/content/gdrive/My Drive/dataset_test/6023.png\n",
      "/content/gdrive/My Drive/dataset_test/C014.png\n",
      "/content/gdrive/My Drive/dataset_test/C033.png\n",
      "/content/gdrive/My Drive/dataset_test/1053.png\n",
      "/content/gdrive/My Drive/dataset_test/4053.png\n",
      "/content/gdrive/My Drive/dataset_test/6051.png\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[array([1]), array([0]), array([1]), array([3]), array([3]), array([0]), array([2]), array([1])]\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('/content/gdrive/My Drive/model/img_class.h5')\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# Path to test data\n",
    "img_dir = \"/content/gdrive/My Drive/dataset_test\"\n",
    "\n",
    "# Iterate over each test image\n",
    "data_path = os.path.join(img_dir, '*g')\n",
    "files = glob.glob(data_path)\n",
    "\n",
    "# Print the files in the dataset_test folder\n",
    "for f in files:\n",
    "    print(f)\n",
    "\n",
    "# Make a prediction and add to results\n",
    "data = []\n",
    "results = []\n",
    "for f1 in files:\n",
    "    img = image.load_img(f1, target_size=(64, 64))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    data.append(img)\n",
    "    result = model.predict(img)\n",
    "    r = np.argmax(result, axis=1)\n",
    "    results.append(r)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cbxo9_gmZcB2",
    "outputId": "06d01cb4-bc20-416a-9495-8131ebf4601c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category 1': 0, 'category 2': 1, 'category 3': 2, 'category 4': 3}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check category labels in training_set\n",
    "train_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GAbiaROpVW-a"
   },
   "outputs": [],
   "source": [
    "# Manually create the test labels \n",
    "test_label = [2, 0, 1, 3, 3, 0, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UdXrtu9AXWwq",
    "outputId": "ae84002f-0a53-4b0a-9889-d63f17d1e827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.5%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert results to a 1D numpy array\n",
    "y_pred_test = np.array([pred[0] for pred in results])\n",
    "\n",
    "# Compute the accuracy score\n",
    "accuracy = accuracy_score(test_label, y_pred_test)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(f'Accuracy: {accuracy*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ESKdz9Tk2MHh",
    "outputId": "dba95641-7420-48bc-cc46-eee8adb98f59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with steps_per_epoch=1 and epochs=1\n",
      "Found 88 images belonging to 4 classes.\n",
      "11/11 [==============================] - 2s 129ms/step - loss: 1.5314 - accuracy: 0.4545\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "*****************************************************************\n",
      "Training model with steps_per_epoch=1 and epochs=2\n",
      "Found 88 images belonging to 4 classes.\n",
      "WARNING: Only 11 batches available for training. Repeating data...\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/2\n",
      "11/11 [==============================] - 7s 567ms/step - loss: 0.9628 - accuracy: 0.6677\n",
      "Epoch 2/2\n",
      "11/11 [==============================] - 5s 453ms/step - loss: 0.1822 - accuracy: 0.9438\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "*****************************************************************\n",
      "Training model with steps_per_epoch=1 and epochs=3\n",
      "Found 88 images belonging to 4 classes.\n",
      "WARNING: Only 11 batches available for training. Repeating data...\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/3\n",
      "11/11 [==============================] - 5s 418ms/step - loss: 0.9948 - accuracy: 0.6311\n",
      "Epoch 2/3\n",
      "11/11 [==============================] - 6s 589ms/step - loss: 0.2059 - accuracy: 0.9500\n",
      "Epoch 3/3\n",
      "11/11 [==============================] - 4s 396ms/step - loss: 0.0741 - accuracy: 0.9719\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "*****************************************************************\n",
      "Training model with steps_per_epoch=2 and epochs=4\n",
      "Found 88 images belonging to 4 classes.\n",
      "WARNING: Only 11 batches available for training. Repeating data...\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/4\n",
      "5/5 [==============================] - 3s 413ms/step - loss: 1.4148 - accuracy: 0.4408\n",
      "Epoch 2/4\n",
      "5/5 [==============================] - 5s 998ms/step - loss: 0.5894 - accuracy: 0.8264\n",
      "Epoch 3/4\n",
      "5/5 [==============================] - 2s 410ms/step - loss: 0.2686 - accuracy: 0.9306\n",
      "Epoch 4/4\n",
      "5/5 [==============================] - 2s 410ms/step - loss: 0.2078 - accuracy: 0.9539\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "*****************************************************************\n",
      "Training model with steps_per_epoch=2 and epochs=5\n",
      "Found 88 images belonging to 4 classes.\n",
      "WARNING: Only 11 batches available for training. Repeating data...\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 3s 411ms/step - loss: 1.2644 - accuracy: 0.4803\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 2s 434ms/step - loss: 0.5899 - accuracy: 0.8542\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.2345 - accuracy: 0.9583\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 3s 605ms/step - loss: 0.1642 - accuracy: 0.9605\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 3s 613ms/step - loss: 0.1201 - accuracy: 0.9514\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "*****************************************************************\n",
      "Training model with steps_per_epoch=2 and epochs=6\n",
      "Found 88 images belonging to 4 classes.\n",
      "WARNING: Only 11 batches available for training. Repeating data...\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/6\n",
      "5/5 [==============================] - 4s 622ms/step - loss: 1.2384 - accuracy: 0.3816\n",
      "Epoch 2/6\n",
      "5/5 [==============================] - 2s 438ms/step - loss: 0.5207 - accuracy: 0.9028\n",
      "Epoch 3/6\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.1481 - accuracy: 0.9722\n",
      "Epoch 4/6\n",
      "5/5 [==============================] - 2s 412ms/step - loss: 0.1122 - accuracy: 0.9671\n",
      "Epoch 5/6\n",
      "5/5 [==============================] - 2s 432ms/step - loss: 0.1103 - accuracy: 0.9861\n",
      "Epoch 6/6\n",
      "5/5 [==============================] - 2s 445ms/step - loss: 0.0739 - accuracy: 0.9792\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "*****************************************************************\n",
      "Training model with steps_per_epoch=3 and epochs=7\n",
      "Found 88 images belonging to 4 classes.\n",
      "WARNING: Only 11 batches available for training. Repeating data...\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/7\n",
      "3/3 [==============================] - 2s 428ms/step - loss: 1.7326 - accuracy: 0.2955\n",
      "Epoch 2/7\n",
      "3/3 [==============================] - 1s 402ms/step - loss: 0.9514 - accuracy: 0.6136\n",
      "Epoch 3/7\n",
      "3/3 [==============================] - 2s 545ms/step - loss: 0.4701 - accuracy: 0.9205\n",
      "Epoch 4/7\n",
      "3/3 [==============================] - 2s 615ms/step - loss: 0.3221 - accuracy: 0.8864\n",
      "Epoch 5/7\n",
      "3/3 [==============================] - 2s 523ms/step - loss: 0.2202 - accuracy: 0.9432\n",
      "Epoch 6/7\n",
      "3/3 [==============================] - 1s 395ms/step - loss: 0.1449 - accuracy: 0.9545\n",
      "Epoch 7/7\n",
      "3/3 [==============================] - 1s 345ms/step - loss: 0.1528 - accuracy: 0.9545\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "*****************************************************************\n",
      "Training model with steps_per_epoch=3 and epochs=8\n",
      "Found 88 images belonging to 4 classes.\n",
      "WARNING: Only 11 batches available for training. Repeating data...\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/8\n",
      "3/3 [==============================] - 2s 402ms/step - loss: 2.4266 - accuracy: 0.2500\n",
      "Epoch 2/8\n",
      "3/3 [==============================] - 1s 382ms/step - loss: 1.7344 - accuracy: 0.2841\n",
      "Epoch 3/8\n",
      "3/3 [==============================] - 1s 392ms/step - loss: 1.0705 - accuracy: 0.4886\n",
      "Epoch 4/8\n",
      "3/3 [==============================] - 1s 400ms/step - loss: 0.7300 - accuracy: 0.9318\n",
      "Epoch 5/8\n",
      "3/3 [==============================] - 2s 618ms/step - loss: 0.4403 - accuracy: 0.9318\n",
      "Epoch 6/8\n",
      "3/3 [==============================] - 2s 620ms/step - loss: 0.2704 - accuracy: 0.9432\n",
      "Epoch 7/8\n",
      "3/3 [==============================] - 2s 516ms/step - loss: 0.2220 - accuracy: 0.9318\n",
      "Epoch 8/8\n",
      "3/3 [==============================] - 1s 368ms/step - loss: 0.1325 - accuracy: 0.9659\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "*****************************************************************\n",
      "Training model with steps_per_epoch=5 and epochs=9\n",
      "Found 88 images belonging to 4 classes.\n",
      "WARNING: Only 11 batches available for training. Repeating data...\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/9\n",
      "2/2 [==============================] - 2s 536ms/step - loss: 1.9098 - accuracy: 0.3281\n",
      "Epoch 2/9\n",
      "2/2 [==============================] - 1s 445ms/step - loss: 1.0420 - accuracy: 0.6071\n",
      "Epoch 3/9\n",
      "2/2 [==============================] - 1s 329ms/step - loss: 0.8588 - accuracy: 0.8393\n",
      "Epoch 4/9\n",
      "2/2 [==============================] - 1s 488ms/step - loss: 0.6143 - accuracy: 0.8750\n",
      "Epoch 5/9\n",
      "2/2 [==============================] - 1s 450ms/step - loss: 0.4058 - accuracy: 0.9286\n",
      "Epoch 6/9\n",
      "2/2 [==============================] - 1s 345ms/step - loss: 0.3169 - accuracy: 0.9107\n",
      "Epoch 7/9\n",
      "2/2 [==============================] - 1s 458ms/step - loss: 0.2625 - accuracy: 0.9062\n",
      "Epoch 8/9\n",
      "2/2 [==============================] - 1s 476ms/step - loss: 0.1671 - accuracy: 0.9643\n",
      "Epoch 9/9\n",
      "2/2 [==============================] - 1s 220ms/step - loss: 0.1919 - accuracy: 0.9107\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "*****************************************************************\n",
      "Training model with steps_per_epoch=5 and epochs=10\n",
      "Found 88 images belonging to 4 classes.\n",
      "WARNING: Only 11 batches available for training. Repeating data...\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 2s 778ms/step - loss: 2.5734 - accuracy: 0.1719\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 1s 609ms/step - loss: 1.1334 - accuracy: 0.5179\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 1s 270ms/step - loss: 1.1941 - accuracy: 0.5893\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 1s 465ms/step - loss: 0.9653 - accuracy: 0.7031\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 1s 562ms/step - loss: 0.7376 - accuracy: 0.8750\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 1s 317ms/step - loss: 0.5656 - accuracy: 0.9464\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 1s 487ms/step - loss: 0.4019 - accuracy: 0.9375\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 1s 486ms/step - loss: 0.3289 - accuracy: 0.8929\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 1s 381ms/step - loss: 0.2250 - accuracy: 0.9286\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 1s 372ms/step - loss: 0.1735 - accuracy: 0.9531\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "*****************************************************************\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "# Define a list to store the results\n",
    "model_results = []\n",
    "\n",
    "# Define a list of tuples for the different combinations of steps_per_epoch and epochs\n",
    "combos = [(1, 1), (1, 2), (1, 3), (2, 4), (2, 5), (2, 6), (3, 7), (3, 8), (5, 9), (5, 10)]\n",
    "\n",
    "# Loop through each combination and train the model\n",
    "for combo in combos:\n",
    "    steps_per_epoch = combo[0]\n",
    "    epochs = combo[1]\n",
    "    print(f'Training model with steps_per_epoch={steps_per_epoch} and epochs={epochs}')\n",
    "\n",
    "    train_set = train_datagen.flow_from_directory(\n",
    "    '/content/gdrive/My Drive/dataset_train',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=8,\n",
    "    classes=['category 1', 'category 2', 'category 3', 'category 4'],\n",
    "    class_mode='categorical')\n",
    "    \n",
    "    # Create and compile the model\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    classifier.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    classifier.add(Flatten())\n",
    "    classifier.add(Dense(units=128, activation='relu'))\n",
    "    classifier.add(Dense(units=4, activation='softmax'))\n",
    "    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Fit the model\n",
    "    batches_per_epoch = len(train_set)//steps_per_epoch\n",
    "    total_batches = batches_per_epoch*epochs\n",
    "    \n",
    "    # Generate repeated training data if necessary\n",
    "    if total_batches > len(train_set):\n",
    "        print(f\"WARNING: Only {len(train_set)} batches available for training. Repeating data...\")\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            '/content/gdrive/My Drive/dataset_train',\n",
    "            target_size=(64, 64),\n",
    "            batch_size=32,\n",
    "            classes=['category 1', 'category 2', 'category 3', 'category 4'],\n",
    "            class_mode='categorical'\n",
    "        )\n",
    "        train_set = itertools.islice(itertools.cycle(train_generator), total_batches)\n",
    "    else:\n",
    "        train_set.reset()\n",
    "\n",
    "    history = classifier.fit(train_set, steps_per_epoch=batches_per_epoch, epochs=epochs)\n",
    "    \n",
    "    # Make a prediction and add to results\n",
    "    data = []\n",
    "    results = []\n",
    "    for f1 in files:\n",
    "        img = image.load_img(f1, target_size=(64, 64))\n",
    "        img = image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        data.append(img)\n",
    "        result = classifier.predict(img)\n",
    "        r = np.argmax(result, axis=1)\n",
    "        results.append(r)\n",
    "\n",
    "    # Convert results to a 1D numpy array\n",
    "    y_pred_test = np.array([pred[0] for pred in results])\n",
    "\n",
    "    # Compute the accuracy score\n",
    "    test_accuracy = accuracy_score(test_label, y_pred_test)\n",
    "    \n",
    "    # Append the results to the list\n",
    "    model_results.append({'Steps per Epoch': steps_per_epoch, 'Epochs': epochs, 'history': history.history, \n",
    "                          'Train Loss': round(sum(history.history['loss']),2), 'Train Accuracy': str(round(statistics.mean(history.history['accuracy'])*100,2))+'%', \n",
    "                          'Test Accuracy': str(test_accuracy*100)+'%'})\n",
    "    print('*****************************************************************')\n",
    "    \n",
    "print('Training complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FukbpdGwY_5M"
   },
   "source": [
    "The total accuracy reported at the end of training is the average of the accuracy calculated at the end of each epoch.\n",
    "During training, the accuracy at each step is calculated based on the predictions made by the model on the current batch of training data. However, the accuracy reported for each epoch is calculated by aggregating the accuracy across all the batches of data that were used in that epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FeZ1iHTp5ZNA",
    "outputId": "86f56edc-2fa0-468a-d764-453147745507"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-fd28604a-8c27-498d-b248-5e4c35354254\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Steps per Epoch</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.73</td>\n",
       "      <td>44.32%</td>\n",
       "      <td>62.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.12</td>\n",
       "      <td>83.63%</td>\n",
       "      <td>87.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.01</td>\n",
       "      <td>88.56%</td>\n",
       "      <td>75.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2.32</td>\n",
       "      <td>79.64%</td>\n",
       "      <td>87.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.62</td>\n",
       "      <td>81.04%</td>\n",
       "      <td>87.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2.49</td>\n",
       "      <td>85.3%</td>\n",
       "      <td>87.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3.99</td>\n",
       "      <td>81.66%</td>\n",
       "      <td>100.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>6.49</td>\n",
       "      <td>76.85%</td>\n",
       "      <td>75.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>6.17</td>\n",
       "      <td>75.37%</td>\n",
       "      <td>87.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>7.48</td>\n",
       "      <td>76.65%</td>\n",
       "      <td>75.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd28604a-8c27-498d-b248-5e4c35354254')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-fd28604a-8c27-498d-b248-5e4c35354254 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-fd28604a-8c27-498d-b248-5e4c35354254');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   Steps per Epoch  Epochs  Train Loss Train Accuracy Test Accuracy\n",
       "0                1       1        1.73         44.32%         62.5%\n",
       "1                1       2        1.12         83.63%         87.5%\n",
       "2                1       3        1.01         88.56%         75.0%\n",
       "3                2       4        2.32         79.64%         87.5%\n",
       "4                2       5        2.62         81.04%         87.5%\n",
       "5                2       6        2.49          85.3%         87.5%\n",
       "6                3       7        3.99         81.66%        100.0%\n",
       "7                3       8        6.49         76.85%         75.0%\n",
       "8                5       9        6.17         75.37%         87.5%\n",
       "9                5      10        7.48         76.65%         75.0%"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.DataFrame([{'Steps per Epoch': d['Steps per Epoch'],\n",
    "                'Epochs': d['Epochs'], 'Train Loss': d['Train Loss'], \n",
    "                'Train Accuracy': d['Train Accuracy'] , 'Test Accuracy': d['Test Accuracy'],} for d in model_results])\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiWxPa4PY_5M"
   },
   "source": [
    "Repeating data in a dataset can lead to overfitting if the model starts to memorize the data instead of learning the underlying patterns. If the model is repeatedly presented with the same examples during training, it can become overly specialized to those specific examples and perform poorly on new, unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYLX9pFFY_5M"
   },
   "source": [
    "### Conceptual Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrBuLPfe50gC"
   },
   "source": [
    "<i>4. Discuss the effect of the following on accuracy and loss (train and test): </i>\n",
    "    \n",
    "* Increasing the steps_per_epoch\n",
    "* Increasing the number of epochs\n",
    "\n",
    "Increasing the steps_per_epoch and the number of epochs can both have an impact on the accuracy and loss of the training and testing process.\n",
    "<b>When the steps_per_epoch is increased, the model is trained on more batches of data in each epoch.</b> This can lead to a more fine-tuned model with better accuracy on the training data. However, it can also increase the risk of overfitting the model to the training data, as it may not generalize well to new, unseen data.\n",
    "<b>When the number of epochs is increased, the model is trained on the same data for longer, allowing it to potentially learn more complex patterns and achieve better accuracy on both the training and testing data.</b>\n",
    "\n",
    "Based on the table, we can observe the effect of changing the hyperparameters steps_per_epoch and epochs on the accuracy of the model.\n",
    "\n",
    "Overall, increasing steps_per_epoch and epochs can improve the accuracy of the model, but only up to a certain extent. After that, further increasing these hyperparameters may not result in a significant improvement in accuracy and may even lead to overfitting of the model.\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>5. Name two uses of zero padding in CNN.</i>\n",
    "\n",
    "<b>Zero padding</b> is a technique used in convolutional neural networks (CNNs) for image processing. \n",
    "Two uses of zero padding in CNN are:\n",
    "* <b>Increasing the size of the output feature maps:</b> Zero padding adds zeros to the border of the input image, which increases the size of the output feature maps. This can be useful when the input size is smaller than the desired output size, as it allows the network to learn more features and capture more information from the image.\n",
    "* <b>Maintaining spatial resolution:</b> Zero padding can help maintain the spatial resolution of the input image in the output feature maps. When a convolution is applied to an input image, the size of the output feature map can be smaller than the input image, especially when using a small filter size or a large stride. By adding zero padding to the input image, the spatial resolution of the output feature map can be preserved.\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>6. What is the use of a 1 x 1 kernel in CNN?</i>\n",
    "A <b>1x1 convolutional kernel</b> is also called a pointwise convolution, which performs a linear transformation on the input data in the channel dimension only. The 1x1 kernel in a CNN can serve several purposes:\n",
    "* <b>Dimensionality Reduction:</b> A 1x1 convolution can be used to reduce the number of channels (or features) in the input data. This can help to reduce the computational complexity of the CNN while preserving important features in the data.\n",
    "* <b>Non-linearity:</b> A 1x1 convolution can introduce non-linearity into the CNN, which can help to improve its performance.\n",
    "* <b>Feature Combination:</b> A 1x1 convolution can be used to combine features from different channels in the input data. This can help to create more complex representations of the input data, which can improve the performance of the CNN.\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>7. What are the advantages of a CNN over a fully connected DNN for this image classification problem?</i>\n",
    "There are several advantages of a convolutional neural network (CNN) over a fully connected deep neural network (DNN) for image classification:\n",
    "\n",
    "* <b>Parameter Efficiency:</b> In a CNN, the same filter/kernel is used to scan different parts of the image, thus reducing the number of parameters required compared to a fully connected DNN. This allows the network to be deeper and more efficient in learning complex features.\n",
    "\n",
    "* <b>Translation Invariance:</b> A CNN can recognize patterns regardless of their position in the image. This is achieved by sharing the same filter/kernel across the entire image. In contrast, a fully connected DNN treats each pixel independently, and therefore is unable to recognize patterns that are shifted or rotated in the image.\n",
    "\n",
    "* <b>Reduced Overfitting:</b> A CNN is less prone to overfitting than a fully connected DNN. This is because the use of pooling layers in a CNN reduces the dimensionality of the feature maps, thus preventing the network from memorizing the training data.\n",
    "\n",
    "* <b>Local Receptive Fields:</b> In a CNN, each neuron is only connected to a small region of the input data, which allows the network to learn local features such as edges and textures more efficiently.\n",
    "\n",
    "Overall, CNNs are more effective at image classification tasks due to their ability to learn and recognize local features in images, their parameter efficiency, and their ability to reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2afe7l0Y_5N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
